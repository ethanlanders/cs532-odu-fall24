<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 2,357 results for author: <span class="mathjax">Wu, J</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Wu, J">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Wu%2C+J&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Wu, J">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15277">arXiv:2409.15277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15277">pdf</a>, <a href="https://arxiv.org/format/2409.15277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+Y">Yunfei Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Juncheng Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+H">Haoqin Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+S">Siwei Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+B">Bingchen Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zong%2C+Y">Yongshuo Zong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+Q">Qiao Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yuyin Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15277v1-abstract-short" style="display: inline;">
        Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI&#39;s o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general langu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15277v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15277v1-abstract-full" style="display: none;">
        Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI&#39;s o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15277v1-abstract-full').style.display = 'none'; document.getElementById('2409.15277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The first four authors contributed equally, project page available at https://ucsc-vlaa.github.io/o1_medicine/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15049">arXiv:2409.15049</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15049">pdf</a>, <a href="https://arxiv.org/format/2409.15049">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PackageIntel: Leveraging Large Language Models for Automated Intelligence Extraction in Package Ecosystems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+W">Wenbo Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Chengwei Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Limin Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiahui Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Z">Zhengzi Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+C">Cheng Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fang%2C+Y">Yong Fang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15049v1-abstract-short" style="display: inline;">
        The rise of malicious packages in public registries poses a significant threat to software supply chain (SSC) security. Although academia and industry employ methods like software composition analysis (SCA) to address this issue, existing approaches often lack timely and comprehensive intelligence updates. This paper introduces PackageIntel, a novel platform that revolutionizes the collection, pro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15049v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15049v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15049v1-abstract-full" style="display: none;">
        The rise of malicious packages in public registries poses a significant threat to software supply chain (SSC) security. Although academia and industry employ methods like software composition analysis (SCA) to address this issue, existing approaches often lack timely and comprehensive intelligence updates. This paper introduces PackageIntel, a novel platform that revolutionizes the collection, processing, and retrieval of malicious package intelligence. By utilizing exhaustive search techniques, snowball sampling from diverse sources, and large language models (LLMs) with specialized prompts, PackageIntel ensures enhanced coverage, timeliness, and accuracy. We have developed a comprehensive database containing 20,692 malicious NPM and PyPI packages sourced from 21 distinct intelligence repositories. Empirical evaluations demonstrate that PackageIntel achieves a precision of 98.6% and an F1 score of 92.0 in intelligence extraction. Additionally, it detects threats on average 70% earlier than leading databases like Snyk and OSV, and operates cost-effectively at $0.094 per intelligence piece. The platform has successfully identified and reported over 1,000 malicious packages in downstream package manager mirror registries. This research provides a robust, efficient, and timely solution for identifying and mitigating threats within the software supply chain ecosystem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15049v1-abstract-full').style.display = 'none'; document.getElementById('2409.15049v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14961">arXiv:2409.14961</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14961">pdf</a>, <a href="https://arxiv.org/format/2409.14961">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UELLM: A Unified and Efficient Approach for LLM Inference Serving
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Y">Yiyuan He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Minxian Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingfeng Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+W">Wanyi Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+K">Kejiang Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+C">Chongzhong Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14961v1-abstract-short" style="display: inline;">
        In the context of Machine Learning as a Service (MLaaS) clouds, the extensive use of Large Language Models (LLMs) often requires efficient management of significant query loads. When providing real-time inference services, several challenges arise. Firstly, increasing the number of GPUs may lead to a decrease in inference speed due to heightened communication overhead, while an inadequate number o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14961v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14961v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14961v1-abstract-full" style="display: none;">
        In the context of Machine Learning as a Service (MLaaS) clouds, the extensive use of Large Language Models (LLMs) often requires efficient management of significant query loads. When providing real-time inference services, several challenges arise. Firstly, increasing the number of GPUs may lead to a decrease in inference speed due to heightened communication overhead, while an inadequate number of GPUs can lead to out-of-memory errors. Secondly, different deployment strategies need to be evaluated to guarantee optimal utilization and minimal inference latency. Lastly, inefficient orchestration of inference queries can easily lead to significant Service Level Objective (SLO) violations. Lastly, inefficient orchestration of inference queries can easily lead to significant Service Level Objective (SLO) violations. To address these challenges, we propose a Unified and Efficient approach for Large Language Model inference serving (UELLM), which consists of three main components: 1) resource profiler, 2) batch scheduler, and 3) LLM deployer. UELLM minimizes resource overhead, reduces inference latency, and lowers SLO violation rates. Compared with state-of-the-art (SOTA) techniques, UELLM reduces the inference latency by 72.3% to 90.3%, enhances GPU utilization by 1.2X to 4.1X, and increases throughput by 1.92X to 4.98X, it can also serve without violating the inference latency SLO.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14961v1-abstract-full').style.display = 'none'; document.getElementById('2409.14961v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 5 figures, ICSOC 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14880">arXiv:2409.14880</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14880">pdf</a>, <a href="https://arxiv.org/format/2409.14880">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        End-to-End Graph Flattening Method for Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+B">Bin Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinze Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiayu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+L">Liang Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sha%2C+J">Jing Sha</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Kai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Shijin Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zhenya Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14880v1-abstract-short" style="display: inline;">
        In recent years, the breakthrough of Large Language Models (LLMs) offers new ideas for achieving universal methods on graph data. The common practice of converting graphs into natural language for LLMs, which refers to graph flattening, exhibits good generalizability and interpretability. However, the poor organization of the textual format results in poor performance in long-distance scenario und&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14880v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14880v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14880v1-abstract-full" style="display: none;">
        In recent years, the breakthrough of Large Language Models (LLMs) offers new ideas for achieving universal methods on graph data. The common practice of converting graphs into natural language for LLMs, which refers to graph flattening, exhibits good generalizability and interpretability. However, the poor organization of the textual format results in poor performance in long-distance scenario understanding. Inspired by human cognitive reasoning habits, we propose a novel method for graph flattening to fit LLMs, termed as End-to-End DAG-Path prompting (EEDP). Experiments on real-world datasets show that EEDP enhances the reasoning performance of LLMs in long-distance scenarios while maintaining excellent performance in short-distance scenarios, demonstrating good robustness in the face of distance variations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14880v1-abstract-full').style.display = 'none'; document.getElementById('2409.14880v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2024 1st International Conference on Computational Linguistics and Natural Language Processing (CLNLP 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14302">arXiv:2409.14302</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14302">pdf</a>, <a href="https://arxiv.org/format/2409.14302">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PretextTrans: Investigating Medical Factual Knowledge Mastery of LLMs with Predicate-text Dual Transformation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yuxuan Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xien Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ning%2C+C">Chen Ning</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Ji Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14302v1-abstract-short" style="display: inline;">
        In the study, we aim to investigate current LLMs&#39; mastery of medical factual knowledge with a dynamic evaluation schema, which can automatically generate multiple test samples for each medical factual knowledge point. Test samples produced directly by LLMs always introduce factual errors and lack diversity in the manner of knowledge expression. To overcome the drawbacks, here we propose a novel ev&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14302v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14302v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14302v1-abstract-full" style="display: none;">
        In the study, we aim to investigate current LLMs&#39; mastery of medical factual knowledge with a dynamic evaluation schema, which can automatically generate multiple test samples for each medical factual knowledge point. Test samples produced directly by LLMs always introduce factual errors and lack diversity in the manner of knowledge expression. To overcome the drawbacks, here we propose a novel evaluation method, Predicate-text Dual Transformation (PretextTrans), by introducing predicate transformations into the dynamic evaluation schema. Specifically, each medical knowledge point is firstly transformed into a predicate expression; then, the predicate expression derives a series of variants through predicate transformations; lastly, the produced predicate variants are transformed back into textual expressions, resulting in a series of test samples with both factual reliability and expression diversity. Using the proposed PretextTrans method, we systematically investigate 12 well-known LLMs&#39; mastery of medical factual knowledge based on two medical datasets. The comparison results show that current LLMs still have significant deficiencies in fully mastering medical knowledge, which may illustrate why current LLMs still perform unsatisfactorily in real-world medical scenarios despite having achieved considerable performance on public benchmarks. Our proposed method serves as an effective solution for evaluation of LLMs in medical domain and offers valuable insights for developing medical-specific LLMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14302v1-abstract-full').style.display = 'none'; document.getElementById('2409.14302v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14057">arXiv:2409.14057</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14057">pdf</a>, <a href="https://arxiv.org/format/2409.14057">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Co-occurrence is not Factual Association in Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+M">Miao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Ji Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14057v1-abstract-short" style="display: inline;">
        Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14057v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14057v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14057v1-abstract-full" style="display: none;">
        Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14057v1-abstract-full').style.display = 'none'; document.getElementById('2409.14057v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13665">arXiv:2409.13665</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13665">pdf</a>, <a href="https://arxiv.org/format/2409.13665">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Fluid Dynamics">physics.flu-dyn</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DiffFluid: Plain Diffusion Models are Effective Predictors of Flow Dynamics
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+D">Dongyu Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianyu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jing Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+H">Hairun Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yue%2C+X">Xiangyu Yue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+S">Shixiang Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13665v1-abstract-short" style="display: inline;">
        We showcase the plain diffusion models with Transformers are effective predictors of fluid dynamics under various working conditions, e.g., Darcy flow and high Reynolds number. Unlike traditional fluid dynamical solvers that depend on complex architectures to extract intricate correlations and learn underlying physical states, our approach formulates the prediction of flow dynamics as the image tr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13665v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13665v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13665v1-abstract-full" style="display: none;">
        We showcase the plain diffusion models with Transformers are effective predictors of fluid dynamics under various working conditions, e.g., Darcy flow and high Reynolds number. Unlike traditional fluid dynamical solvers that depend on complex architectures to extract intricate correlations and learn underlying physical states, our approach formulates the prediction of flow dynamics as the image translation problem and accordingly leverage the plain diffusion model to tackle the problem. This reduction in model design complexity does not compromise its ability to capture complex physical states and geometric features of fluid dynamical equations, leading to high-precision solutions. In preliminary tests on various fluid-related benchmarks, our DiffFluid achieves consistent state-of-the-art performance, particularly in solving the Navier-Stokes equations in fluid dynamics, with a relative precision improvement of +44.8%. In addition, we achieved relative improvements of +14.0% and +11.3% in the Darcy flow equation and the airfoil problem with Euler&#39;s equation, respectively. Code will be released at https://github.com/DongyuLUO/DiffFluid upon acceptance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13665v1-abstract-full').style.display = 'none'; document.getElementById('2409.13665v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13486">arXiv:2409.13486</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13486">pdf</a>, <a href="https://arxiv.org/format/2409.13486">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3641519.3657493">10.1145/3641519.3657493 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DiffSound: Differentiable Modal Sound Rendering and Inverse Rendering for Diverse Inference Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+X">Xutong Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+C">Chenxi Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+R">Ruohan Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Guoping Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S">Sheng Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13486v1-abstract-short" style="display: inline;">
        Accurately estimating and simulating the physical properties of objects from real-world sound recordings is of great practical importance in the fields of vision, graphics, and robotics. However, the progress in these directions has been limited -- prior differentiable rigid or soft body simulation techniques cannot be directly applied to modal sound synthesis due to the high sampling rate of audi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13486v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13486v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13486v1-abstract-full" style="display: none;">
        Accurately estimating and simulating the physical properties of objects from real-world sound recordings is of great practical importance in the fields of vision, graphics, and robotics. However, the progress in these directions has been limited -- prior differentiable rigid or soft body simulation techniques cannot be directly applied to modal sound synthesis due to the high sampling rate of audio, while previous audio synthesizers often do not fully model the accurate physical properties of the sounding objects. We propose DiffSound, a differentiable sound rendering framework for physics-based modal sound synthesis, which is based on an implicit shape representation, a new high-order finite element analysis module, and a differentiable audio synthesizer. Our framework can solve a wide range of inverse problems thanks to the differentiability of the entire pipeline, including physical parameter estimation, geometric shape reasoning, and impact position prediction. Experimental results demonstrate the effectiveness of our approach, highlighting its ability to accurately reproduce the target sound in a physics-based manner. DiffSound serves as a valuable tool for various sound synthesis and analysis applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13486v1-abstract-full').style.display = 'none'; document.getElementById('2409.13486v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 10 figures. Published in Siggraph 2024. Project page: https://hellojxt.github.io/DiffSound/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13321">arXiv:2409.13321</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13321">pdf</a>, <a href="https://arxiv.org/format/2409.13321">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SLaVA-CXR: Small Language and Vision Assistant for Chest X-ray Report Automation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinge Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+Y">Yunsoo Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+D">Daqian Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cliffton%2C+D">David Cliffton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+F">Fenglin Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Honghan Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13321v1-abstract-short" style="display: inline;">
        Inspired by the success of large language models (LLMs), there is growing research interest in developing LLMs in the medical domain to assist clinicians. However, for hospitals, using closed-source commercial LLMs involves privacy issues, and developing open-source public LLMs requires large-scale computational resources, which are usually limited, especially in resource-efficient regions and low&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13321v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13321v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13321v1-abstract-full" style="display: none;">
        Inspired by the success of large language models (LLMs), there is growing research interest in developing LLMs in the medical domain to assist clinicians. However, for hospitals, using closed-source commercial LLMs involves privacy issues, and developing open-source public LLMs requires large-scale computational resources, which are usually limited, especially in resource-efficient regions and low-income countries. We propose an open-source Small Language and Vision Assistant (SLaVA-CXR) that can be used for Chest X-Ray report automation. To efficiently train a small assistant, we first propose the Re$^3$Training method, which simulates the cognitive development of radiologists and optimizes the model in the Recognition, Reasoning, and Reporting training manner. Then, we introduce a data synthesis method, RADEX, which can generate a high-quality and diverse training corpus with privacy regulation compliance. The extensive experiments show that our SLaVA-CXR built on a 2.7B backbone not only outperforms but also achieves 6 times faster inference efficiency than previous state-of-the-art larger models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13321v1-abstract-full').style.display = 'none'; document.getElementById('2409.13321v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13156">arXiv:2409.13156</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13156">pdf</a>, <a href="https://arxiv.org/format/2409.13156">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RRM: Robust Reward Model Training Mitigates Reward Hacking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+T">Tianqi Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiong%2C+W">Wei Xiong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+J">Jie Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Lichang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junru Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Joshi%2C+R">Rishabh Joshi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yang Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+J">Jiaming Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+Z">Zhen Qin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+T">Tianhe Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohn%2C+D">Daniel Sohn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Makarova%2C+A">Anastasiia Makarova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jeremiah Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Piot%2C+B">Bilal Piot</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ittycheriah%2C+A">Abe Ittycheriah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+A">Aviral Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saleh%2C+M">Mohammad Saleh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13156v1-abstract-short" style="display: inline;">
        Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13156v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13156v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13156v1-abstract-full" style="display: none;">
        Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). Our RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13156v1-abstract-full').style.display = 'none'; document.getElementById('2409.13156v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13059">arXiv:2409.13059</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13059">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comprehensive Overview of Artificial Intelligence Applications in Modern Industries
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Weng%2C+Y">Yijie Weng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianhao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kelly%2C+T">Tara Kelly</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Johnson%2C+W">William Johnson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13059v1-abstract-short" style="display: inline;">
        Artificial Intelligence (AI) is fundamentally reshaping various industries by enhancing decision-making processes, optimizing operations, and unlocking new opportunities for innovation. This paper explores the applications of AI across four key sectors: healthcare, finance, manufacturing, and retail. Each section delves into the specific challenges faced by these industries, the AI technologies em&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13059v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13059v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13059v1-abstract-full" style="display: none;">
        Artificial Intelligence (AI) is fundamentally reshaping various industries by enhancing decision-making processes, optimizing operations, and unlocking new opportunities for innovation. This paper explores the applications of AI across four key sectors: healthcare, finance, manufacturing, and retail. Each section delves into the specific challenges faced by these industries, the AI technologies employed to address them, and the measurable impact on business outcomes and societal welfare. We also discuss the implications of AI integration, including ethical considerations, the future trajectory of AI development, and its potential to drive economic growth while posing challenges that need to be managed responsibly.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13059v1-abstract-full').style.display = 'none'; document.getElementById('2409.13059v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12467">arXiv:2409.12467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12467">pdf</a>, <a href="https://arxiv.org/format/2409.12467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhen Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+X">Xingjian Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinlin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bai%2C+L">Long Bai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+Z">Zhen Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+H">Hongliang Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ourselin%2C+S">Sebastien Ourselin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Hongbin Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12467v1-abstract-short" style="display: inline;">
        Surgical phase recognition is critical for assisting surgeons in understanding surgical videos. Existing studies focused more on online surgical phase recognition, by leveraging preceding frames to predict the current frame. Despite great progress, they formulated the task as a series of frame-wise classification, which resulted in a lack of global context of the entire procedure and incoherent pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12467v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12467v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12467v1-abstract-full" style="display: none;">
        Surgical phase recognition is critical for assisting surgeons in understanding surgical videos. Existing studies focused more on online surgical phase recognition, by leveraging preceding frames to predict the current frame. Despite great progress, they formulated the task as a series of frame-wise classification, which resulted in a lack of global context of the entire procedure and incoherent predictions. Moreover, besides online analysis, accurate offline surgical phase recognition is also in significant clinical need for retrospective analysis, and existing online algorithms do not fully analyze the entire video, thereby limiting accuracy in offline analysis. To overcome these challenges and enhance both online and offline inference capabilities, we propose a universal Surgical Phase Localization Network, named SurgPLAN++, with the principle of temporal detection. To ensure a global understanding of the surgical procedure, we devise a phase localization strategy for SurgPLAN++ to predict phase segments across the entire video through phase proposals. For online analysis, to generate high-quality phase proposals, SurgPLAN++ incorporates a data augmentation strategy to extend the streaming video into a pseudo-complete video through mirroring, center-duplication, and down-sampling. For offline analysis, SurgPLAN++ capitalizes on its global phase prediction framework to continuously refine preceding predictions during each online inference step, thereby significantly improving the accuracy of phase recognition. We perform extensive experiments to validate the effectiveness, and our SurgPLAN++ achieves remarkable performance in both online and offline modes, which outperforms state-of-the-art methods. The source code is available at https://github.com/lxj22/SurgPLAN-Plus.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12467v1-abstract-full').style.display = 'none'; document.getElementById('2409.12467v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11653">arXiv:2409.11653</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11653">pdf</a>, <a href="https://arxiv.org/format/2409.11653">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+Q">Qian Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+J">Jiangrui Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Q">Qiyuan Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zepeng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hongxia Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+Y">Yiwen Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+J">Jiajuan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11653v1-abstract-short" style="display: inline;">
        Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep learning tasks, which reduces the need for human labor. Previous studies primarily focus on effectively utilising the labelled and unlabeled data to improve performance. However, we observe that how to select samples for labelling also significantly impacts performance, particularly under extremely low-budget settings. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11653v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11653v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11653v1-abstract-full" style="display: none;">
        Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep learning tasks, which reduces the need for human labor. Previous studies primarily focus on effectively utilising the labelled and unlabeled data to improve performance. However, we observe that how to select samples for labelling also significantly impacts performance, particularly under extremely low-budget settings. The sample selection task in SSL has been under-explored for a long time. To fill in this gap, we propose a Representative and Diverse Sample Selection approach (RDSS). By adopting a modified Frank-Wolfe algorithm to minimise a novel criterion $$-Maximum Mean Discrepancy ($$-MMD), RDSS samples a representative and diverse subset for annotation from the unlabeled data. We demonstrate that minimizing $$-MMD enhances the generalization ability of low-budget learning. Experimental results show that RDSS consistently improves the performance of several popular SSL frameworks and outperforms the state-of-the-art sample selection approaches used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even with constrained annotation budgets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11653v1-abstract-full').style.display = 'none'; document.getElementById('2409.11653v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under Review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11174">arXiv:2409.11174</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11174">pdf</a>, <a href="https://arxiv.org/format/2409.11174">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Identifying Influential nodes in Brain Networks via Self-Supervised Graph-Transformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+Y">Yanqing Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+D">Di Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Haiyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+E">Enze Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+S">Sigang Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinru Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xuhui Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+G">Geng Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+X">Xi Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+T">Tuo Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shu Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11174v1-abstract-short" style="display: inline;">
        Studying influential nodes (I-nodes) in brain networks is of great significance in the field of brain imaging. Most existing studies consider brain connectivity hubs as I-nodes. However, this approach relies heavily on prior knowledge from graph theory, which may overlook the intrinsic characteristics of the brain network, especially when its architecture is not fully understood. In contrast, self&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11174v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11174v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11174v1-abstract-full" style="display: none;">
        Studying influential nodes (I-nodes) in brain networks is of great significance in the field of brain imaging. Most existing studies consider brain connectivity hubs as I-nodes. However, this approach relies heavily on prior knowledge from graph theory, which may overlook the intrinsic characteristics of the brain network, especially when its architecture is not fully understood. In contrast, self-supervised deep learning can learn meaningful representations directly from the data. This approach enables the exploration of I-nodes for brain networks, which is also lacking in current studies. This paper proposes a Self-Supervised Graph Reconstruction framework based on Graph-Transformer (SSGR-GT) to identify I-nodes, which has three main characteristics. First, as a self-supervised model, SSGR-GT extracts the importance of brain nodes to the reconstruction. Second, SSGR-GT uses Graph-Transformer, which is well-suited for extracting features from brain graphs, combining both local and global characteristics. Third, multimodal analysis of I-nodes uses graph-based fusion technology, combining functional and structural brain information. The I-nodes we obtained are distributed in critical areas such as the superior frontal lobe, lateral parietal lobe, and lateral occipital lobe, with a total of 56 identified across different experiments. These I-nodes are involved in more brain networks than other regions, have longer fiber connections, and occupy more central positions in structural connectivity. They also exhibit strong connectivity and high node efficiency in both functional and structural networks. Furthermore, there is a significant overlap between the I-nodes and both the structural and functional rich-club. These findings enhance our understanding of the I-nodes within the brain network, and provide new insights for future research in further understanding the brain working mechanisms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11174v1-abstract-full').style.display = 'none'; document.getElementById('2409.11174v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10504">arXiv:2409.10504</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10504">pdf</a>, <a href="https://arxiv.org/format/2409.10504">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">John Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+D">David Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jimeng Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10504v1-abstract-short" style="display: inline;">
        Predicting high-dimensional or extreme multilabels, such as in medical coding, requires both accuracy and interpretability. Existing works often rely on local interpretability methods, failing to provide comprehensive explanations of the overall mechanism behind each label prediction within a multilabel set. We propose a mechanistic interpretability module called DIctionary Label Attention (\metho&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10504v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10504v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10504v1-abstract-full" style="display: none;">
        Predicting high-dimensional or extreme multilabels, such as in medical coding, requires both accuracy and interpretability. Existing works often rely on local interpretability methods, failing to provide comprehensive explanations of the overall mechanism behind each label prediction within a multilabel set. We propose a mechanistic interpretability module called DIctionary Label Attention (\method) that disentangles uninterpretable dense embeddings into a sparse embedding space, where each nonzero element (a dictionary feature) represents a globally learned medical concept. Through human evaluations, we show that our sparse embeddings are more human understandable than its dense counterparts by at least 50 percent. Our automated dictionary feature identification pipeline, leveraging large language models (LLMs), uncovers thousands of learned medical concepts by examining and summarizing the highest activating tokens for each dictionary feature. We represent the relationships between dictionary features and medical codes through a sparse interpretable matrix, enhancing the mechanistic and global understanding of the model&#39;s predictions while maintaining competitive performance and scalability without extensive human annotation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10504v1-abstract-full').style.display = 'none'; document.getElementById('2409.10504v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10330">arXiv:2409.10330</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10330">pdf</a>, <a href="https://arxiv.org/format/2409.10330">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lai%2C+S">Songning Lai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+T">Tianlang Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+H">Hongru Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+L">Lijie Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiemin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+N">Ninghui Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guan%2C+R">Runwei Guan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+H">Haicheng Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhenning Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yue%2C+Y">Yutao Yue</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10330v1-abstract-short" style="display: inline;">
        Recent advancements in autonomous driving have seen a paradigm shift towards end-to-end learning paradigms, which map sensory inputs directly to driving actions, thereby enhancing the robustness and adaptability of autonomous vehicles. However, these models often sacrifice interpretability, posing significant challenges to trust, safety, and regulatory compliance. To address these issues, we intro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10330v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10330v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10330v1-abstract-full" style="display: none;">
        Recent advancements in autonomous driving have seen a paradigm shift towards end-to-end learning paradigms, which map sensory inputs directly to driving actions, thereby enhancing the robustness and adaptability of autonomous vehicles. However, these models often sacrifice interpretability, posing significant challenges to trust, safety, and regulatory compliance. To address these issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving, a comprehensive framework designed to improve the dependability and stability of explanations in end-to-end unsupervised autonomous driving models. Our work specifically targets the inherent instability problems observed in the Driving through the Concept Gridlock (DCG) model, which undermine the trustworthiness of its explanations and decision-making processes. We define four key attributes of DRIVE: consistent interpretability, stable interpretability, consistent output, and stable output. These attributes collectively ensure that explanations remain reliable and robust across different scenarios and perturbations. Through extensive empirical evaluations, we demonstrate the effectiveness of our framework in enhancing the stability and dependability of explanations, thereby addressing the limitations of current models. Our contributions include an in-depth analysis of the dependability issues within the DCG model, a rigorous definition of DRIVE with its fundamental properties, a framework to implement DRIVE, and novel metrics for evaluating the dependability of concept-based explainable autonomous driving models. These advancements lay the groundwork for the development of more reliable and trusted autonomous driving systems, paving the way for their broader acceptance and deployment in real-world applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10330v1-abstract-full').style.display = 'none'; document.getElementById('2409.10330v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09997">arXiv:2409.09997</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09997">pdf</a>, <a href="https://arxiv.org/format/2409.09997">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ViewActive: Active viewpoint optimization from a single image
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiayi Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+X">Xiaomin Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+B">Botao He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fermuller%2C+C">Cornelia Fermuller</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aloimonos%2C+Y">Yiannis Aloimonos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09997v2-abstract-short" style="display: inline;">
        When observing objects, humans benefit from their spatial visualization and mental rotation ability to envision potential optimal viewpoints based on the current observation. This capability is crucial for enabling robots to achieve efficient and robust scene perception during operation, as optimal viewpoints provide essential and informative features for accurately representing scenes in 2D image&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09997v2-abstract-full').style.display = 'inline'; document.getElementById('2409.09997v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09997v2-abstract-full" style="display: none;">
        When observing objects, humans benefit from their spatial visualization and mental rotation ability to envision potential optimal viewpoints based on the current observation. This capability is crucial for enabling robots to achieve efficient and robust scene perception during operation, as optimal viewpoints provide essential and informative features for accurately representing scenes in 2D images, thereby enhancing downstream tasks.
  To endow robots with this human-like active viewpoint optimization capability, we propose ViewActive, a modernized machine learning approach drawing inspiration from aspect graph, which provides viewpoint optimization guidance based solely on the current 2D image input. Specifically, we introduce the 3D Viewpoint Quality Field (VQF), a compact and consistent representation for viewpoint quality distribution similar to an aspect graph, composed of three general-purpose viewpoint quality metrics: self-occlusion ratio, occupancy-aware surface normal entropy, and visual entropy. We utilize pre-trained image encoders to extract robust visual and semantic features, which are then decoded into the 3D VQF, allowing our model to generalize effectively across diverse objects, including unseen categories.The lightweight ViewActive network (72 FPS on a single GPU) significantly enhances the performance of state-of-the-art object recognition pipelines and can be integrated into real-time motion planning for robotic applications. Our code and dataset are available here: https://github.com/jiayi-wu-umd/ViewActive
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09997v2-abstract-full').style.display = 'none'; document.getElementById('2409.09997v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09615">arXiv:2409.09615</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09615">pdf</a>, <a href="https://arxiv.org/format/2409.09615">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Text Annotation through Rationale-Driven Collaborative Few-Shot Prompting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianfei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xubin Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+W">Weijia Jia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09615v1-abstract-short" style="display: inline;">
        The traditional data annotation process is often labor-intensive, time-consuming, and susceptible to human bias, which complicates the management of increasingly complex datasets. This study explores the potential of large language models (LLMs) as automated data annotators to improve efficiency and consistency in annotation tasks. By employing rationale-driven collaborative few-shot prompting tec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09615v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09615v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09615v1-abstract-full" style="display: none;">
        The traditional data annotation process is often labor-intensive, time-consuming, and susceptible to human bias, which complicates the management of increasingly complex datasets. This study explores the potential of large language models (LLMs) as automated data annotators to improve efficiency and consistency in annotation tasks. By employing rationale-driven collaborative few-shot prompting techniques, we aim to improve the performance of LLMs in text annotation. We conduct a rigorous evaluation of six LLMs across four benchmark datasets, comparing seven distinct methodologies. Our results demonstrate that collaborative methods consistently outperform traditional few-shot techniques and other baseline approaches, particularly in complex annotation tasks. Our work provides valuable insights and a robust framework for leveraging collaborative learning methods to tackle challenging text annotation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09615v1-abstract-full').style.display = 'none'; document.getElementById('2409.09615v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09410">arXiv:2409.09410</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09410">pdf</a>, <a href="https://arxiv.org/format/2409.09410">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributed Invariant Kalman Filter for Object-level Multi-robot Pose SLAM
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Haoying Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+Q">Qingcheng Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Haoran Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yanglin Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junfeng Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09410v1-abstract-short" style="display: inline;">
        Cooperative localization and target tracking are essential for multi-robot systems to implement high-level tasks. To this end, we propose a distributed invariant Kalman filter based on covariance intersection for effective multi-robot pose estimation. The paper utilizes the object-level measurement models, which have condensed information further reducing the communication burden. Besides, by mode&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09410v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09410v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09410v1-abstract-full" style="display: none;">
        Cooperative localization and target tracking are essential for multi-robot systems to implement high-level tasks. To this end, we propose a distributed invariant Kalman filter based on covariance intersection for effective multi-robot pose estimation. The paper utilizes the object-level measurement models, which have condensed information further reducing the communication burden. Besides, by modeling states on special Lie groups, the better linearity and consistency of the invariant Kalman filter structure can be stressed. We also use a combination of CI and KF to avoid overly confident or conservative estimates in multi-robot systems with intricate and unknown correlations, and some level of robot degradation is acceptable through multi-robot collaboration. The simulation and real data experiment validate the practicability and superiority of the proposed algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09410v1-abstract-full').style.display = 'none'; document.getElementById('2409.09410v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08273">arXiv:2409.08273</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08273">pdf</a>, <a href="https://arxiv.org/format/2409.08273">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hand-Object Interaction Pretraining from Videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Singh%2C+H+G">Himanshu Gaurav Singh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Loquercio%2C+A">Antonio Loquercio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sferrazza%2C+C">Carmelo Sferrazza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jane Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qi%2C+H">Haozhi Qi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abbeel%2C+P">Pieter Abbeel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Malik%2C+J">Jitendra Malik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08273v1-abstract-short" style="display: inline;">
        We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic ba&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08273v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08273v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08273v1-abstract-full" style="display: none;">
        We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \url{https://hgaurav2k.github.io/hop/}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08273v1-abstract-full').style.display = 'none'; document.getElementById('2409.08273v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08207">arXiv:2409.08207</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08207">pdf</a>, <a href="https://arxiv.org/format/2409.08207">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiafu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+Y">Ying Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+J">Jinlong Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mao%2C+X">Xiaofeng Mao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chi%2C+M">Mingmin Chi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+M">Mufeng Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+B">Bo Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+Y">Yun Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08207v1-abstract-short" style="display: inline;">
        Recently, methods like Zero-1-2-3 have focused on single-view based 3D reconstruction and have achieved remarkable success. However, their predictions for unseen areas heavily rely on the inductive bias of large-scale pretrained diffusion models. Although subsequent work, such as DreamComposer, attempts to make predictions more controllable by incorporating additional views, the results remain unr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08207v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08207v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08207v1-abstract-full" style="display: none;">
        Recently, methods like Zero-1-2-3 have focused on single-view based 3D reconstruction and have achieved remarkable success. However, their predictions for unseen areas heavily rely on the inductive bias of large-scale pretrained diffusion models. Although subsequent work, such as DreamComposer, attempts to make predictions more controllable by incorporating additional views, the results remain unrealistic due to feature entanglement in the vanilla latent space, including factors such as lighting, material, and structure. To address these issues, we introduce the Visual Isotropy 3D Reconstruction Model (VI3DRM), a diffusion-based sparse views 3D reconstruction model that operates within an ID consistent and perspective-disentangled 3D latent space. By facilitating the disentanglement of semantic information, color, material properties and lighting, VI3DRM is capable of generating highly realistic images that are indistinguishable from real photographs. By leveraging both real and synthesized images, our approach enables the accurate construction of pointmaps, ultimately producing finely textured meshes or point clouds. On the NVS task, tested on the GSO dataset, VI3DRM significantly outperforms state-of-the-art method DreamComposer, achieving a PSNR of 38.61, an SSIM of 0.929, and an LPIPS of 0.027. Code will be made available upon publication.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08207v1-abstract-full').style.display = 'none'; document.getElementById('2409.08207v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08202">arXiv:2409.08202</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08202">pdf</a>, <a href="https://arxiv.org/format/2409.08202">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What Makes a Maze Look Like a Maze?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+J">Joy Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mao%2C+J">Jiayuan Mao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goodman%2C+N+D">Noah D. Goodman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08202v1-abstract-short" style="display: inline;">
        A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08202v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08202v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08202v1-abstract-full" style="display: none;">
        A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as tree branches), they still struggle to make sense of such visual abstractions (e.g., how an arrangement of tree branches may form the walls of a maze). To address this challenge, we introduce Deep Schema Grounding (DSG), a framework that leverages explicit structured representations of visual abstractions for grounding and reasoning. At the core of DSG are schemas--dependency graph descriptions of abstract concepts that decompose them into more primitive-level symbols. DSG uses large language models to extract schemas, then hierarchically grounds concrete to abstract components of the schema onto images with vision-language models. The grounded schema is used to augment visual abstraction understanding. We systematically evaluate DSG and different methods in reasoning on our new Visual Abstractions Dataset, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans. We show that DSG significantly improves the abstract visual reasoning performance of vision-language models, and is a step toward human-aligned understanding of visual abstractions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08202v1-abstract-full').style.display = 'none'; document.getElementById('2409.08202v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06206">arXiv:2409.06206</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06206">pdf</a>, <a href="https://arxiv.org/format/2409.06206">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AgileIR: Memory-Efficient Group Shifted Windows Attention for Agile Image Restoration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+H">Hongyi Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rahman%2C+M+M">Mohammad Mahdinur Rahman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Akhtar%2C+M+S">Mohammad Shahid Akhtar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jie Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingyu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fang%2C+Z">Zhili Fang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06206v1-abstract-short" style="display: inline;">
        Image Transformers show a magnificent success in Image Restoration tasks. Nevertheless, most of transformer-based models are strictly bounded by exorbitant memory occupancy. Our goal is to reduce the memory consumption of Swin Transformer and at the same time speed up the model during training process. Thus, we introduce AgileIR, group shifted attention mechanism along with window attention, which&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06206v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06206v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06206v1-abstract-full" style="display: none;">
        Image Transformers show a magnificent success in Image Restoration tasks. Nevertheless, most of transformer-based models are strictly bounded by exorbitant memory occupancy. Our goal is to reduce the memory consumption of Swin Transformer and at the same time speed up the model during training process. Thus, we introduce AgileIR, group shifted attention mechanism along with window attention, which sparsely simplifies the model in architecture. We propose Group Shifted Window Attention (GSWA) to decompose Shift Window Multi-head Self Attention (SW-MSA) and Window Multi-head Self Attention (W-MSA) into groups across their attention heads, contributing to shrinking memory usage in back propagation. In addition to that, we keep shifted window masking and its shifted learnable biases during training, in order to induce the model interacting across windows within the channel. We also re-allocate projection parameters to accelerate attention matrix calculation, which we found a negligible decrease in performance. As a result of experiment, compared with our baseline SwinIR and other efficient quantization models, AgileIR keeps the performance still at 32.20 dB on Set5 evaluation dataset, exceeding other methods with tailor-made efficient methods and saves over 50% memory while a large batch size is employed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06206v1-abstract-full').style.display = 'none'; document.getElementById('2409.06206v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06201">arXiv:2409.06201</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06201">pdf</a>, <a href="https://arxiv.org/format/2409.06201">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Fluid Dynamics">physics.flu-dyn</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3687996">10.1145/3687996 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Eulerian Vortex Method on Flow Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Sinan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Y">Yitong Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+M">Molin Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+H">Hong-Xing Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Junwei Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Duowen Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Komura%2C+T">Taku Komura</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+B">Bo Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06201v2-abstract-short" style="display: inline;">
        We present an Eulerian vortex method based on the theory of flow maps to simulate the complex vortical motions of incompressible fluids. Central to our method is the novel incorporation of the flow-map transport equations for line elements, which, in combination with a bi-directional marching scheme for flow maps, enables the high-fidelity Eulerian advection of vorticity variables. The fundamental&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06201v2-abstract-full').style.display = 'inline'; document.getElementById('2409.06201v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06201v2-abstract-full" style="display: none;">
        We present an Eulerian vortex method based on the theory of flow maps to simulate the complex vortical motions of incompressible fluids. Central to our method is the novel incorporation of the flow-map transport equations for line elements, which, in combination with a bi-directional marching scheme for flow maps, enables the high-fidelity Eulerian advection of vorticity variables. The fundamental motivation is that, compared to impulse $\mathbf{m}$, which has been recently bridged with flow maps to encouraging results, vorticity $\boldsymbol$ promises to be preferable for its numerical stability and physical interpretability. To realize the full potential of this novel formulation, we develop a new Poisson solving scheme for vorticity-to-velocity reconstruction that is both efficient and able to accurately handle the coupling near solid boundaries. We demonstrate the efficacy of our approach with a range of vortex simulation examples, including leapfrog vortices, vortex collisions, cavity flow, and the formation of complex vortical structures due to solid-fluid interactions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06201v2-abstract-full').style.display = 'none'; document.getElementById('2409.06201v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ACM Transactions on Graphics (SIGGRAPH Asia 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06029">arXiv:2409.06029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06029">pdf</a>, <a href="https://arxiv.org/format/2409.06029">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SongCreator: Lyrics-based Universal Song Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+S">Shun Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yixuan Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+B">Boshi Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lam%2C+M+W+Y">Max W. Y. Lam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+F">Feng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Hangyu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingcheng Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+S">Shiyin Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Z">Zhiyong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06029v1-abstract-short" style="display: inline;">
        Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06029v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06029v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06029v1-abstract-full" style="display: none;">
        Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world. In this light, we propose SongCreator, a song-generation system designed to tackle this challenge. The model features two novel designs: a meticulously designed dual-sequence language model (DSLM) to capture the information of vocals and accompaniment for song generation, and an additional attention mask strategy for DSLM, which allows our model to understand, generate and edit songs, making it suitable for various song-related generation tasks. Extensive experiments demonstrate the effectiveness of SongCreator by achieving state-of-the-art or competitive performances on all eight tasks. Notably, it surpasses previous works by a large margin in lyrics-to-song and lyrics-to-vocals. Additionally, it is able to independently control the acoustic conditions of the vocals and accompaniment in the generated song through different prompts, exhibiting its potential applicability. Our samples are available at https://songcreator.github.io/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06029v1-abstract-full').style.display = 'none'; document.getElementById('2409.06029v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">work in progress</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05885">arXiv:2409.05885</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05885">pdf</a>, <a href="https://arxiv.org/format/2409.05885">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Dual-Path neural network model to construct the flame nonlinear thermoacoustic response in the time domain
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiawei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+T">Teng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nan%2C+J">Jiaqi Nan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+L">Lijun Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jingxuan Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05885v1-abstract-short" style="display: inline;">
        Traditional numerical simulation methods require substantial computational resources to accurately determine the complete nonlinear thermoacoustic response of flames to various perturbation frequencies and amplitudes. In this paper, we have developed deep learning algorithms that can construct a comprehensive flame nonlinear response from limited numerical simulation data. To achieve this, we prop&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05885v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05885v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05885v1-abstract-full" style="display: none;">
        Traditional numerical simulation methods require substantial computational resources to accurately determine the complete nonlinear thermoacoustic response of flames to various perturbation frequencies and amplitudes. In this paper, we have developed deep learning algorithms that can construct a comprehensive flame nonlinear response from limited numerical simulation data. To achieve this, we propose using a frequency-sweeping data type as the training dataset, which incorporates a rich array of learnable information within a constrained dataset. To enhance the precision in learning flame nonlinear response patterns from the training data, we introduce a Dual-Path neural network. This network consists of a Chronological Feature Path and a Temporal Detail Feature Path. The Dual-Path network is specifically designed to focus intensively on the temporal characteristics of velocity perturbation sequences, yielding more accurate flame response patterns and enhanced generalization capabilities. Validations confirm that our approach can accurately model flame nonlinear responses, even under conditions of significant nonlinearity, and exhibits robust generalization capabilities across various test scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05885v1-abstract-full').style.display = 'none'; document.getElementById('2409.05885v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages 14figures, 1 supplemmentary meterial</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05093">arXiv:2409.05093</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05093">pdf</a>, <a href="https://arxiv.org/format/2409.05093">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CloudNativeSim: a toolkit for modeling and simulation of cloud-native applications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingfeng Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Minxian Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Y">Yiyuan He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+K">Kejiang Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+C">Chengzhong Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05093v1-abstract-short" style="display: inline;">
        Cloud-native applications are increasingly becoming popular in modern software design. Employing a microservice-based architecture into these applications is a prevalent strategy that enhances system availability and flexibility. However, cloud-native applications also introduce new challenges, such as frequent inter-service communication and the complexity of managing heterogeneous codebases and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05093v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05093v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05093v1-abstract-full" style="display: none;">
        Cloud-native applications are increasingly becoming popular in modern software design. Employing a microservice-based architecture into these applications is a prevalent strategy that enhances system availability and flexibility. However, cloud-native applications also introduce new challenges, such as frequent inter-service communication and the complexity of managing heterogeneous codebases and hardware, resulting in unpredictable complexity and dynamism. Furthermore, as applications scale, only limited research teams or enterprises possess the resources for large-scale deployment and testing, which impedes progress in the cloud-native domain. To address these challenges, we propose CloudNativeSim, a simulator for cloud-native applications with a microservice-based architecture. CloudNativeSim offers several key benefits: (i) comprehensive and dynamic modeling for cloud-native applications, (ii) an extended simulation framework with new policy interfaces for scheduling cloud-native applications, and (iii) support for customized application scenarios and user feedback based on Quality of Service (QoS) metrics. CloudNativeSim can be easily deployed on standard computers to manage a high volume of requests and services. Its performance was validated through a case study, demonstrating higher than 94.5% accuracy in terms of response time. The study further highlights the feasibility of CloudNativeSim by illustrating the effects of various scaling policies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05093v1-abstract-full').style.display = 'none'; document.getElementById('2409.05093v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04961">arXiv:2409.04961</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04961">pdf</a>, <a href="https://arxiv.org/format/2409.04961">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Heterogeneous LiDAR Dataset for Benchmarking Robust Localization in Diverse Degenerate Scenarios
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhiqiang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qi%2C+Y">Yuhua Qi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+D">Dapeng Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuang%2C+X">Xuebin Zhuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hongbo Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+X">Xiangcheng Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+K">Kelin Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+P">Peng Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04961v2-abstract-short" style="display: inline;">
        The ability to estimate pose and generate maps using 3D LiDAR significantly enhances robotic system autonomy. However, existing open-source datasets lack representation of geometrically degenerate environments, limiting the development and benchmarking of robust LiDAR SLAM algorithms. To address this gap, we introduce GEODE, a comprehensive multi-LiDAR, multi-scenario dataset specifically designed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04961v2-abstract-full').style.display = 'inline'; document.getElementById('2409.04961v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04961v2-abstract-full" style="display: none;">
        The ability to estimate pose and generate maps using 3D LiDAR significantly enhances robotic system autonomy. However, existing open-source datasets lack representation of geometrically degenerate environments, limiting the development and benchmarking of robust LiDAR SLAM algorithms. To address this gap, we introduce GEODE, a comprehensive multi-LiDAR, multi-scenario dataset specifically designed to include real-world geometrically degenerate environments. GEODE comprises 64 trajectories spanning over 64 kilometers across seven diverse settings with varying degrees of degeneracy. The data was meticulously collected to promote the development of versatile algorithms by incorporating various LiDAR sensors, stereo cameras, IMUs, and diverse motion conditions. We evaluate state-of-the-art SLAM approaches using the GEODE dataset to highlight current limitations in LiDAR SLAM techniques. This extensive dataset will be publicly available at https://geode.github.io, supporting further advancements in LiDAR-based SLAM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04961v2-abstract-full').style.display = 'none'; document.getElementById('2409.04961v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 9 figures, 6 tables. Submitted for IJRR dataset paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04937">arXiv:2409.04937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04937">pdf</a>, <a href="https://arxiv.org/format/2409.04937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CONNECTOR: Enhancing the Traceability of Decentralized Bridge Applications via Automatic Cross-chain Transaction Association
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+D">Dan Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+Y">Yuxin Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Z">Ziye Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nan%2C+Y">Yuhong Nan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Z">Zibin Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04937v1-abstract-short" style="display: inline;">
        Decentralized bridge applications are important software that connects various blockchains and facilitates cross-chain asset transfer in the decentralized finance (DeFi) ecosystem which currently operates in a multi-chain environment. Cross-chain transaction association identifies and matches unique transactions executed by bridge DApps, which is important research to enhance the traceability of c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04937v1-abstract-full').style.display = 'inline'; document.getElementById('2409.04937v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04937v1-abstract-full" style="display: none;">
        Decentralized bridge applications are important software that connects various blockchains and facilitates cross-chain asset transfer in the decentralized finance (DeFi) ecosystem which currently operates in a multi-chain environment. Cross-chain transaction association identifies and matches unique transactions executed by bridge DApps, which is important research to enhance the traceability of cross-chain bridge DApps. However, existing methods rely entirely on unobservable internal ledgers or APIs, violating the open and decentralized properties of blockchain. In this paper, we analyze the challenges of this issue and then present CONNECTOR, an automated cross-chain transaction association analysis method based on bridge smart contracts. Specifically, CONNECTOR first identifies deposit transactions by extracting distinctive and generic features from the transaction traces of bridge contracts. With the accurate deposit transactions, CONNECTOR mines the execution logs of bridge contracts to achieve withdrawal transaction matching. We conduct real-world experiments on different types of bridges to demonstrate the effectiveness of CONNECTOR. The experiment demonstrates that CONNECTOR successfully identifies 100% deposit transactions, associates 95.81% withdrawal transactions, and surpasses methods for CeFi bridges. Based on the association results, we obtain interesting findings about cross-chain transaction behaviors in DeFi bridges and analyze the tracing abilities of CONNECTOR to assist the DeFi bridge apps.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04937v1-abstract-full').style.display = 'none'; document.getElementById('2409.04937v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04927">arXiv:2409.04927</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04927">pdf</a>, <a href="https://arxiv.org/format/2409.04927">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Just ASR + LLM? A Study on Speech Large Language Models&#39; Ability to Identify and Understand Speaker in Spoken Dialogue
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junkai Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+X">Xulin Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+B">Bo-Ru Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+X">Xilin Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mesgarani%2C+N">Nima Mesgarani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hasegawa-Johnson%2C+M">Mark Hasegawa-Johnson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ostendorf%2C+M">Mari Ostendorf</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04927v2-abstract-short" style="display: inline;">
        In recent years, we have observed a rapid advancement in speech language models (SpeechLLMs), catching up with humans&#39; listening and reasoning abilities. SpeechLLMs have demonstrated impressive spoken dialog question-answering (SQA) performance in benchmarks like Gaokao, the English listening test of the college entrance exam in China, which seemingly requires understanding both the spoken content&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04927v2-abstract-full').style.display = 'inline'; document.getElementById('2409.04927v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04927v2-abstract-full" style="display: none;">
        In recent years, we have observed a rapid advancement in speech language models (SpeechLLMs), catching up with humans&#39; listening and reasoning abilities. SpeechLLMs have demonstrated impressive spoken dialog question-answering (SQA) performance in benchmarks like Gaokao, the English listening test of the college entrance exam in China, which seemingly requires understanding both the spoken content and voice characteristics of speakers in a conversation. However, after carefully examining Gaokao&#39;s questions, we find the correct answers to many questions can be inferred from the conversation transcript alone, i.e.\ without speaker segmentation and identification. Our evaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and our proposed &#34;What Do You Like?&#34; dataset shows a significantly higher accuracy in these context-based questions than in identity-critical questions, which can only be answered reliably with correct speaker identification. The results and analysis suggest that when solving SQA, the current SpeechLLMs exhibit limited speaker awareness from the audio and behave similarly to an LLM reasoning from the conversation transcription without sound. We propose that tasks focused on identity-critical questions could offer a more accurate evaluation framework of SpeechLLMs in SQA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04927v2-abstract-full').style.display = 'none'; document.getElementById('2409.04927v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04421">arXiv:2409.04421</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04421">pdf</a>, <a href="https://arxiv.org/format/2409.04421">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiaxing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ning%2C+L">Lin Ning</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+L">Luyang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+H">Harrison Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+N">Neo Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Prakash%2C+S">Sushant Prakash</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Banion%2C+S">Shawn O&#39;Banion</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Green%2C+B">Bradley Green</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+J">Jun Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04421v1-abstract-short" style="display: inline;">
        LLM-powered personalization agent systems employ Large Language Models (LLMs) to predict users&#39; behavior from their past activities. However, their effectiveness often hinges on the ability to effectively leverage extensive, long user historical data due to its inherent noise and length of such data. Existing pretrained LLMs may generate summaries that are concise but lack the necessary context fo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04421v1-abstract-full').style.display = 'inline'; document.getElementById('2409.04421v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04421v1-abstract-full" style="display: none;">
        LLM-powered personalization agent systems employ Large Language Models (LLMs) to predict users&#39; behavior from their past activities. However, their effectiveness often hinges on the ability to effectively leverage extensive, long user historical data due to its inherent noise and length of such data. Existing pretrained LLMs may generate summaries that are concise but lack the necessary context for downstream tasks, hindering their utility in personalization systems. To address these challenges, we introduce Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to generate concise, human-readable user summaries that are optimized for downstream task performance. By maximizing the usefulness of the generated summaries, RLPF effectively distills extensive user history data while preserving essential information for downstream tasks. Our empirical evaluation demonstrates significant improvements in both extrinsic downstream task utility and intrinsic summary quality, surpassing baseline methods by up to 22% on downstream task performance and achieving an up to 84.59% win rate on Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable 74% reduction in context length while improving performance on 16 out of 19 unseen tasks and/or datasets, showcasing its generalizability. This approach offers a promising solution for enhancing LLM personalization by effectively transforming long, noisy user histories into informative and human-readable representations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04421v1-abstract-full').style.display = 'none'; document.getElementById('2409.04421v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04270">arXiv:2409.04270</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04270">pdf</a>, <a href="https://arxiv.org/format/2409.04270">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advancing Automated Knowledge Transfer in Evolutionary Multitasking via Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yuxiao Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+X">Xuebin Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Shenghao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jibin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+L">Liang Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+K+C">Kay Chen Tan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04270v1-abstract-short" style="display: inline;">
        Evolutionary Multi-task Optimization (EMTO) is a paradigm that leverages knowledge transfer across simultaneously optimized tasks for enhanced search performance. To facilitate EMTO&#39;s performance, various knowledge transfer models have been developed for specific optimization tasks. However, designing these models often requires substantial expert knowledge. Recently, large language models (LLMs)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04270v1-abstract-full').style.display = 'inline'; document.getElementById('2409.04270v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04270v1-abstract-full" style="display: none;">
        Evolutionary Multi-task Optimization (EMTO) is a paradigm that leverages knowledge transfer across simultaneously optimized tasks for enhanced search performance. To facilitate EMTO&#39;s performance, various knowledge transfer models have been developed for specific optimization tasks. However, designing these models often requires substantial expert knowledge. Recently, large language models (LLMs) have achieved remarkable success in autonomous programming, aiming to produce effective solvers for specific problems. In this work, a LLM-based optimization paradigm is introduced to establish an autonomous model factory for generating knowledge transfer models, ensuring effective and efficient knowledge transfer across various optimization tasks. To evaluate the performance of the proposed method, we conducted comprehensive empirical studies comparing the knowledge transfer model generated by the LLM with existing state-of-the-art knowledge transfer methods. The results demonstrate that the generated model is able to achieve superior or competitive performance against hand-crafted knowledge transfer models in terms of both efficiency and effectiveness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04270v1-abstract-full').style.display = 'none'; document.getElementById('2409.04270v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 11 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03782">arXiv:2409.03782</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03782">pdf</a>, <a href="https://arxiv.org/format/2409.03782">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Assessing the Uncertainty and Robustness of Object Detection Models for Detecting Stickers on Laptops
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+C">Chengjie Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiahui Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ali%2C+S">Shaukat Ali</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Olsen%2C+M+L">Mikkel Labori Olsen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03782v1-abstract-short" style="display: inline;">
        Refurbishing laptops extends their lives while contributing to reducing electronic waste, which promotes building a sustainable future. To this end, the Danish Technological Institute (DTI) focuses on the research and development of several applications, including laptop refurbishing. This has several steps, including cleaning, which involves identifying and removing stickers from laptop surfaces.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03782v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03782v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03782v1-abstract-full" style="display: none;">
        Refurbishing laptops extends their lives while contributing to reducing electronic waste, which promotes building a sustainable future. To this end, the Danish Technological Institute (DTI) focuses on the research and development of several applications, including laptop refurbishing. This has several steps, including cleaning, which involves identifying and removing stickers from laptop surfaces. DTI trained six sticker detection models (SDMs) based on open-source object detection models to identify such stickers precisely so these stickers can be removed automatically. However, given the diversity in types of stickers (e.g., shapes, colors, locations), identification of the stickers is highly uncertain, thereby requiring explicit quantification of uncertainty associated with the identified stickers. Such uncertainty quantification can help reduce risks in removing stickers, which, for example, could otherwise result in damaging laptop surfaces. For uncertainty quantification, we adopted the Monte Carlo Dropout method to evaluate the six SDMs from DTI using three datasets: the original image dataset from DTI and two datasets generated with vision language models, i.e., DALL-E-3 and Stable Diffusion-3. In addition, we presented novel robustness metrics concerning detection accuracy and uncertainty to assess the robustness of the SDMs based on adversarial datasets generated from the three datasets using a dense adversary method. Our evaluation results show that different SDMs perform differently regarding different metrics. Based on the results, we provide SDM selection guidelines and lessons learned from various perspectives.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03782v1-abstract-full').style.display = 'none'; document.getElementById('2409.03782v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 6 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03685">arXiv:2409.03685</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03685">pdf</a>, <a href="https://arxiv.org/format/2409.03685">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        View-Invariant Policy Learning via Zero-Shot Novel View Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+S">Stephen Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wulfe%2C+B">Blake Wulfe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sargent%2C+K">Kyle Sargent</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+K">Katherine Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zakharov%2C+S">Sergey Zakharov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guizilini%2C+V">Vitor Guizilini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03685v1-abstract-short" style="display: inline;">
        Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: obs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03685v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03685v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03685v1-abstract-full" style="display: none;">
        Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03685v1-abstract-full').style.display = 'none'; document.getElementById('2409.03685v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to CoRL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03550">arXiv:2409.03550</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03550">pdf</a>, <a href="https://arxiv.org/format/2409.03550">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+Q">Qianlong Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Miao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+Y">Yuzhang Shang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianlong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+Y">Yan Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nie%2C+L">Liqiang Nie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03550v1-abstract-short" style="display: inline;">
        Diffusion models (DMs) have demonstrated exceptional generative capabilities across various areas, while they are hindered by slow inference speeds and high computational demands during deployment. The most common way to accelerate DMs involves reducing the number of denoising steps during generation, achieved through faster sampling solvers or knowledge distillation (KD). In contrast to prior app&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03550v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03550v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03550v1-abstract-full" style="display: none;">
        Diffusion models (DMs) have demonstrated exceptional generative capabilities across various areas, while they are hindered by slow inference speeds and high computational demands during deployment. The most common way to accelerate DMs involves reducing the number of denoising steps during generation, achieved through faster sampling solvers or knowledge distillation (KD). In contrast to prior approaches, we propose a novel method that transfers the capability of large pretrained DMs to faster architectures. Specifically, we employ KD in a distinct manner to compress DMs by distilling their generative ability into more rapid variants. Furthermore, considering that the source data is either unaccessible or too enormous to store for current generative models, we introduce a new paradigm for their distillation without source data, termed Data-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our established DKDM framework comprises two main components: 1) a DKDM objective that uses synthetic denoising data produced by pretrained DMs to optimize faster DMs without source data, and 2) a dynamic iterative distillation method that flexibly organizes the synthesis of denoising data, preventing it from slowing down the optimization process as the generation is slow. To our knowledge, this is the first attempt at using KD to distill DMs into any architecture in a data-free manner. Importantly, our DKDM is orthogonal to most existing acceleration methods, such as denoising step reduction, quantization and pruning. Experiments show that our DKDM is capable of deriving 2x faster DMs with performance remaining on par with the baseline. Notably, our DKDM enables pretrained DMs to function as &#34;datasets&#34; for training new DMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03550v1-abstract-full').style.display = 'none'; document.getElementById('2409.03550v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03218">arXiv:2409.03218</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03218">pdf</a>, <a href="https://arxiv.org/format/2409.03218">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Application Research On Real-Time Perception Of Device Performance Status
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhe Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhen Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianwen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+W">Wangzhong Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yidong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+Z">Zihua Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+D">Dian Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Hongchen Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+B">Bo Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+J">Jiaojiao Fu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03218v1-abstract-short" style="display: inline;">
        In order to accurately identify the performance status of mobile devices and finely adjust the user experience, a real-time performance perception evaluation method based on TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) combined with entropy weighting method and time series model construction was studied. After collecting the performance characteristics of various mobile&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03218v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03218v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03218v1-abstract-full" style="display: none;">
        In order to accurately identify the performance status of mobile devices and finely adjust the user experience, a real-time performance perception evaluation method based on TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) combined with entropy weighting method and time series model construction was studied. After collecting the performance characteristics of various mobile devices, the device performance profile was fitted by using PCA (principal component analysis) dimensionality reduction and feature engineering methods such as descriptive time series analysis. The ability of performance features and profiles to describe the real-time performance status of devices was understood and studied by applying the TOPSIS method and multi-level weighting processing. A time series model was constructed for the feature set under objective weighting, and multiple sensitivity (real-time, short-term, long-term) performance status perception results were provided to obtain real-time performance evaluation data and long-term stable performance prediction data. Finally, by configuring dynamic AB experiments and overlaying fine-grained power reduction strategies, the usability of the method was verified, and the accuracy of device performance status identification and prediction was compared with the performance of the profile features including dimensionality reduction time series modeling, TOPSIS method and entropy weighting method, subjective weighting, HMA method. The results show that accurate real-time performance perception results can greatly enhance business value, and this research has application effectiveness and certain forward-looking significance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03218v1-abstract-full').style.display = 'none'; document.getElementById('2409.03218v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03164">arXiv:2409.03164</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03164">pdf</a>, <a href="https://arxiv.org/format/2409.03164">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Scalable Matrix Visualization for Understanding Tree Ensemble Classifiers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+W">Weikai Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+J">Jun Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Changjian Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ming%2C+Y">Yao Ming</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+F">Fan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Hui Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shixia Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03164v1-abstract-short" style="display: inline;">
        The high performance of tree ensemble classifiers benefits from a large set of rules, which, in turn, makes the models hard to understand. To improve interpretability, existing methods extract a subset of rules for approximation using model reduction techniques. However, by focusing on the reduced rule set, these methods often lose fidelity and ignore anomalous rules that, despite their infrequenc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03164v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03164v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03164v1-abstract-full" style="display: none;">
        The high performance of tree ensemble classifiers benefits from a large set of rules, which, in turn, makes the models hard to understand. To improve interpretability, existing methods extract a subset of rules for approximation using model reduction techniques. However, by focusing on the reduced rule set, these methods often lose fidelity and ignore anomalous rules that, despite their infrequency, play crucial roles in real-world applications. This paper introduces a scalable visual analysis method to explain tree ensemble classifiers that contain tens of thousands of rules. The key idea is to address the issue of losing fidelity by adaptively organizing the rules as a hierarchy rather than reducing them. To ensure the inclusion of anomalous rules, we develop an anomaly-biased model reduction method to prioritize these rules at each hierarchical level. Synergized with this hierarchical organization of rules, we develop a matrix-based hierarchical visualization to support exploration at different levels of detail. Our quantitative experiments and case studies demonstrate how our method fosters a deeper understanding of both common and anomalous rules, thereby enhancing interpretability without sacrificing comprehensiveness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03164v1-abstract-full').style.display = 'none'; document.getElementById('2409.03164v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02968">arXiv:2409.02968</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02968">pdf</a>, <a href="https://arxiv.org/format/2409.02968">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Comprehensive Survey of Blockchain Scalability: Shaping Inner-Chain and Inter-Chain Perspectives
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Baochao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+L">Liyuan Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hao Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+J">Juncheng Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+D">Dengcheng Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xiulong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jie Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jianrong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Keqiu Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02968v1-abstract-short" style="display: inline;">
        Blockchain is widely applied in logistics, finance, and agriculture. As single blockchain users grow, scalability becomes crucial. However, existing works lack a comprehensive summary of blockchain scalability. They focus on single chains or cross-chain technologies. This survey summarizes scalability across the physical and logical layers, as well as inner-chain, inter-chain, and technology dimen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02968v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02968v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02968v1-abstract-full" style="display: none;">
        Blockchain is widely applied in logistics, finance, and agriculture. As single blockchain users grow, scalability becomes crucial. However, existing works lack a comprehensive summary of blockchain scalability. They focus on single chains or cross-chain technologies. This survey summarizes scalability across the physical and logical layers, as well as inner-chain, inter-chain, and technology dimensions. The physical layer covers data and protocols, while the logical layer represents blockchain architecture. Each component is analyzed from inner-chain and inter-chain perspectives, considering technological factors. The aim is to enhance researchers&#39; understanding of blockchain&#39;s architecture, data, and protocols to advance scalability research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02968v1-abstract-full').style.display = 'none'; document.getElementById('2409.02968v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02834">arXiv:2409.02834</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02834">pdf</a>, <a href="https://arxiv.org/format/2409.02834">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+W">Wentao Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+Q">Qianjun Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhuo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Ji Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Jie Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+A">Aimin Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Q">Qin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+B">Bo Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+L">Liang He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02834v2-abstract-short" style="display: inline;">
        Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02834v2-abstract-full').style.display = 'inline'; document.getElementById('2409.02834v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02834v2-abstract-full" style="display: none;">
        Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02834v2-abstract-full').style.display = 'none'; document.getElementById('2409.02834v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02598">arXiv:2409.02598</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02598">pdf</a>, <a href="https://arxiv.org/format/2409.02598">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+W">Wenwu Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinlin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhen Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Q">Qingxiang Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Miao Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+Z">Zhen Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Hongbin Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02598v1-abstract-short" style="display: inline;">
        Vision-based surgical navigation has received increasing attention due to its non-invasive, cost-effective, and flexible advantages. In particular, a critical element of the vision-based navigation system is tracking surgical instruments. Compared with 2D instrument tracking methods, 3D instrument tracking has broader value in clinical practice, but is also more challenging due to weak texture, oc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02598v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02598v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02598v1-abstract-full" style="display: none;">
        Vision-based surgical navigation has received increasing attention due to its non-invasive, cost-effective, and flexible advantages. In particular, a critical element of the vision-based navigation system is tracking surgical instruments. Compared with 2D instrument tracking methods, 3D instrument tracking has broader value in clinical practice, but is also more challenging due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models for 3D registration. To solve these challenges, we propose the SurgTrack, a two-stage 3D instrument tracking method for CAD-free and robust real-world applications. In the first registration stage, we incorporate an Instrument Signed Distance Field (SDF) modeling the 3D representation of instruments, achieving CAD-freed 3D registration. Due to this, we can obtain the location and orientation of instruments in the 3D space by matching the video stream with the registered SDF model. In the second tracking stage, we devise a posture graph optimization module, leveraging the historical tracking results of the posture memory pool to optimize the tracking results and improve the occlusion robustness. Furthermore, we collect the Instrument3D dataset to comprehensively evaluate the 3D tracking of surgical instruments. The extensive experiments validate the superiority and scalability of our SurgTrack, by outperforming the state-of-the-arts with a remarkable improvement. The code and dataset are available at https://github.com/wenwucode/SurgTrack.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02598v1-abstract-full').style.display = 'none'; document.getElementById('2409.02598v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02530">arXiv:2409.02530</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02530">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chih-Yuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jun-Ting Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chan Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+M">Ming-Yen Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+Y">Yihuang Kang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02530v1-abstract-short" style="display: inline;">
        The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02530v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02530v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02530v1-abstract-full" style="display: none;">
        The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02530v1-abstract-full').style.display = 'none'; document.getElementById('2409.02530v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This preprint version includes corrections of typographical errors related to numerical values in Table 2, which were present in the version published at the BDH workshop in MIPR 2024. These corrections do not affect the overall conclusions of the study</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02310">arXiv:2409.02310</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02310">pdf</a>, <a href="https://arxiv.org/format/2409.02310">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Geometry-aware Feature Matching for Large-Scale Structure from Motion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+G">Gonglin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinsen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Haiwei Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Teng%2C+W">Wenbin Teng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Z">Zhiyuan Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+A">Andrew Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+R">Rongjun Qin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yajie Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02310v2-abstract-short" style="display: inline;">
        Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02310v2-abstract-full').style.display = 'inline'; document.getElementById('2409.02310v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02310v2-abstract-full" style="display: none;">
        Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry cues in addition to color cues. This helps fill gaps when there is less overlap in large-scale scenarios. Our method formulates geometric verification as an optimization problem, guiding feature matching within detector-free methods and using sparse correspondences from detector-based methods as anchor points. By enforcing geometric constraints via the Sampson Distance, our approach ensures that the denser correspondences from detector-free methods are geometrically consistent and more accurate. This hybrid strategy significantly improves correspondence density and accuracy, mitigates multi-view inconsistencies, and leads to notable advancements in camera pose accuracy and point cloud density. It outperforms state-of-the-art feature matching methods on benchmark datasets and enables feature matching in challenging extreme large-scale settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02310v2-abstract-full').style.display = 'none'; document.getElementById('2409.02310v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.01661">arXiv:2409.01661</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.01661">pdf</a>, <a href="https://arxiv.org/format/2409.01661">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        $S^2$NeRF: Privacy-preserving Training Framework for NeRF
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+B">Bokang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yanglin Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhikun Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jinglan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lingying Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junfeng Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.01661v1-abstract-short" style="display: inline;">
        Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and graphics, facilitating novel view synthesis and influencing sectors like extended reality and e-commerce. However, NeRF&#39;s dependence on extensive data collection, including sensitive scene image data, introduces significant privacy risks when users upload this data for model training. To address this concern, we first propose&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01661v1-abstract-full').style.display = 'inline'; document.getElementById('2409.01661v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.01661v1-abstract-full" style="display: none;">
        Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and graphics, facilitating novel view synthesis and influencing sectors like extended reality and e-commerce. However, NeRF&#39;s dependence on extensive data collection, including sensitive scene image data, introduces significant privacy risks when users upload this data for model training. To address this concern, we first propose SplitNeRF, a training framework that incorporates split learning (SL) techniques to enable privacy-preserving collaborative model training between clients and servers without sharing local data. Despite its benefits, we identify vulnerabilities in SplitNeRF by developing two attack methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which exploit the shared gradient data and a few leaked scene images to reconstruct private scene information. To counter these threats, we introduce $S^2$NeRF, secure SplitNeRF that integrates effective defense mechanisms. By introducing decaying noise related to the gradient norm into the shared gradient information, $S^2$NeRF preserves privacy while maintaining a high utility of the NeRF model. Our extensive evaluations across multiple datasets demonstrate the effectiveness of $S^2$NeRF against privacy breaches, confirming its viability for secure NeRF training in sensitive applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01661v1-abstract-full').style.display = 'none'; document.getElementById('2409.01661v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in the ACM Conference on Computer and Communications Security (CCS&#39;24), October 14-18, 2024, Salt Lake City, UT, USA</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.01184">arXiv:2409.01184</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.01184">pdf</a>, <a href="https://arxiv.org/format/2409.01184">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PitVis-2023 Challenge: Workflow Recognition in videos of Endoscopic Pituitary Surgery
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Das%2C+A">Adrito Das</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+D+Z">Danyal Z. Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Psychogyios%2C+D">Dimitrios Psychogyios</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yitong Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hanrahan%2C+J+G">John G. Hanrahan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vasconcelos%2C+F">Francisco Vasconcelos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+Y">You Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhen Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinlin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+X">Xiaoyang Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+G">Guoyan Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qayyum%2C+A">Abdul Qayyum</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mazher%2C+M">Moona Mazher</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Razzak%2C+I">Imran Razzak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+T">Tianbin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+J">Jin Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+J">Junjun He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=P%C5%82otka%2C+S">Szymon Potka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaleta%2C+J">Joanna Kaleta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yamlahi%2C+A">Amine Yamlahi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jund%2C+A">Antoine Jund</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Godau%2C+P">Patrick Godau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kondo%2C+S">Satoshi Kondo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kasai%2C+S">Satoshi Kasai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hirasawa%2C+K">Kousuke Hirasawa</a>
      , et al. (7 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.01184v1-abstract-short" style="display: inline;">
        The field of computer vision applied to videos of minimally invasive surgery is ever-growing. Workflow recognition pertains to the automated recognition of various aspects of a surgery: including which surgical steps are performed; and which surgical instruments are used. This information can later be used to assist clinicians when learning the surgery; during live surgery; and when writing operat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01184v1-abstract-full').style.display = 'inline'; document.getElementById('2409.01184v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.01184v1-abstract-full" style="display: none;">
        The field of computer vision applied to videos of minimally invasive surgery is ever-growing. Workflow recognition pertains to the automated recognition of various aspects of a surgery: including which surgical steps are performed; and which surgical instruments are used. This information can later be used to assist clinicians when learning the surgery; during live surgery; and when writing operation notes. The Pituitary Vision (PitVis) 2023 Challenge tasks the community to step and instrument recognition in videos of endoscopic pituitary surgery. This is a unique task when compared to other minimally invasive surgeries due to the smaller working space, which limits and distorts vision; and higher frequency of instrument and step switching, which requires more precise model predictions. Participants were provided with 25-videos, with results presented at the MICCAI-2023 conference as part of the Endoscopic Vision 2023 Challenge in Vancouver, Canada, on 08-Oct-2023. There were 18-submissions from 9-teams across 6-countries, using a variety of deep learning models. A commonality between the top performing models was incorporating spatio-temporal and multi-task methods, with greater than 50% and 10% macro-F1-score improvement over purely spacial single-task models in step and instrument recognition respectively. The PitVis-2023 Challenge therefore demonstrates state-of-the-art computer vision models in minimally invasive surgery are transferable to a new dataset, with surgery specific techniques used to enhance performance, progressing the field further. Benchmark results are provided in the paper, and the dataset is publicly available at: https://doi.org/10.5522/04/26531686.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01184v1-abstract-full').style.display = 'none'; document.getElementById('2409.01184v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.00330">arXiv:2409.00330</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.00330">pdf</a>, <a href="https://arxiv.org/format/2409.00330">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GMFL-Net: A Global Multi-geometric Feature Learning Network for Repetitive Action Counting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jun Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinying Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Qiming Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+F">Feifei Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.00330v1-abstract-short" style="display: inline;">
        With the continuous development of deep learning, the field of repetitive action counting is gradually gaining notice from many researchers. Extraction of pose keypoints using human pose estimation networks is proven to be an effective pose-level method. However, existing pose-level methods suffer from the shortcomings that the single coordinate is not stable enough to handle action distortions du&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00330v1-abstract-full').style.display = 'inline'; document.getElementById('2409.00330v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.00330v1-abstract-full" style="display: none;">
        With the continuous development of deep learning, the field of repetitive action counting is gradually gaining notice from many researchers. Extraction of pose keypoints using human pose estimation networks is proven to be an effective pose-level method. However, existing pose-level methods suffer from the shortcomings that the single coordinate is not stable enough to handle action distortions due to changes in camera viewpoints, thus failing to accurately identify salient poses, and is vulnerable to misdetection during the transition from the exception to the actual action. To overcome these problems, we propose a simple but efficient Global Multi-geometric Feature Learning Network (GMFL-Net). Specifically, we design a MIA-Module that aims to improve information representation by fusing multi-geometric features, and learning the semantic similarity among the input multi-geometric features. Then, to improve the feature representation from a global perspective, we also design a GBFL-Module that enhances the inter-dependencies between point-wise and channel-wise elements and combines them with the rich local information generated by the MIA-Module to synthesise a comprehensive and most representative global feature representation. In addition, considering the insufficient existing dataset, we collect a new dataset called Countix-Fitness-pose (https://github.com/Wantong66/Countix-Fitness) which contains different cycle lengths and exceptions, a test set with longer duration, and annotate it with fine-grained annotations at the pose-level. We also add two new action classes, namely lunge and rope push-down. Finally, extensive experiments on the challenging RepCount-pose, UCFRep-pose, and Countix-Fitness-pose benchmarks show that our proposed GMFL-Net achieves state-of-the-art performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00330v1-abstract-full').style.display = 'none'; document.getElementById('2409.00330v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.17135">arXiv:2408.17135</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.17135">pdf</a>, <a href="https://arxiv.org/format/2408.17135">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Temporal and Interactive Modeling for Efficient Human-Human Motion Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yabiao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Shuo Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jiangning Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+K">Ke Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiafu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Z">Zhengkai Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yong Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.17135v1-abstract-short" style="display: inline;">
        Human-human motion generation is essential for understanding humans as social beings. Although several transformer-based methods have been proposed, they typically model each individual separately and overlook the causal relationships in temporal motion sequences. Furthermore, the attention mechanism in transformers exhibits quadratic computational complexity, significantly reducing their efficien&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.17135v1-abstract-full').style.display = 'inline'; document.getElementById('2408.17135v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.17135v1-abstract-full" style="display: none;">
        Human-human motion generation is essential for understanding humans as social beings. Although several transformer-based methods have been proposed, they typically model each individual separately and overlook the causal relationships in temporal motion sequences. Furthermore, the attention mechanism in transformers exhibits quadratic computational complexity, significantly reducing their efficiency when processing long sequences. In this paper, we introduce TIM (Temporal and Interactive Modeling), an efficient and effective approach that presents the pioneering human-human motion generation model utilizing RWKV. Specifically, we first propose Causal Interactive Injection to leverage the temporal properties of motion sequences and avoid non-causal and cumbersome modeling. Then we present Role-Evolving Mixing to adjust to the ever-evolving roles throughout the interaction. Finally, to generate smoother and more rational motion, we design Localized Pattern Amplification to capture short-term motion patterns. Extensive experiments on InterHuman demonstrate that our method achieves superior performance. Notably, TIM has achieved state-of-the-art results using only 32% of InterGen&#39;s trainable parameters. Code will be available soon. Homepage: https://aigc-explorer.github.io/TIM-page/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.17135v1-abstract-full').style.display = 'none'; document.getElementById('2408.17135v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Homepage: https://aigc-explorer.github.io/TIM-page/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.17005">arXiv:2408.17005</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.17005">pdf</a>, <a href="https://arxiv.org/format/2408.17005">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Camera Exposure Control for Visual Odometry via Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shuyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+J">Jinhao He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yilong Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+J">Jie Yuan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.17005v1-abstract-short" style="display: inline;">
        The stability of visual odometry (VO) systems is undermined by degraded image quality, especially in environments with significant illumination changes. This study employs a deep reinforcement learning (DRL) framework to train agents for exposure control, aiming to enhance imaging performance in challenging conditions. A lightweight image simulator is developed to facilitate the training process,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.17005v1-abstract-full').style.display = 'inline'; document.getElementById('2408.17005v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.17005v1-abstract-full" style="display: none;">
        The stability of visual odometry (VO) systems is undermined by degraded image quality, especially in environments with significant illumination changes. This study employs a deep reinforcement learning (DRL) framework to train agents for exposure control, aiming to enhance imaging performance in challenging conditions. A lightweight image simulator is developed to facilitate the training process, enabling the diversification of image exposure and sequence trajectory. This setup enables completely offline training, eliminating the need for direct interaction with camera hardware and the real environments. Different levels of reward functions are crafted to enhance the VO systems, equipping the DRL agents with varying intelligence. Extensive experiments have shown that our exposure control agents achieve superior efficiency-with an average inference duration of 1.58 ms per frame on a CPU-and respond more quickly than traditional feedback control schemes. By choosing an appropriate reward function, agents acquire an intelligent understanding of motion trends and anticipate future illumination changes. This predictive capability allows VO systems to deliver more stable and precise odometry results. The codes and datasets are available at https://github.com/ShuyangUni/drl_exposure_ctrl.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.17005v1-abstract-full').style.display = 'none'; document.getElementById('2408.17005v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.16966">arXiv:2408.16966</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.16966">pdf</a>, <a href="https://arxiv.org/format/2408.16966">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UserSumBench: A Benchmark Framework for Evaluating User Summarization Approaches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+N">Neo Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ning%2C+L">Lin Ning</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiaxing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+L">Luyang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+J">Jun Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Banion%2C+S">Shawn O&#39;Banion</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Green%2C+B">Bradley Green</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.16966v2-abstract-short" style="display: inline;">
        Large language models (LLMs) have shown remarkable capabilities in generating user summaries from a long list of raw user activity data. These summaries capture essential user information such as preferences and interests, and therefore are invaluable for LLM-based personalization applications, such as explainable recommender systems. However, the development of new summarization techniques is hin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.16966v2-abstract-full').style.display = 'inline'; document.getElementById('2408.16966v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.16966v2-abstract-full" style="display: none;">
        Large language models (LLMs) have shown remarkable capabilities in generating user summaries from a long list of raw user activity data. These summaries capture essential user information such as preferences and interests, and therefore are invaluable for LLM-based personalization applications, such as explainable recommender systems. However, the development of new summarization techniques is hindered by the lack of ground-truth labels, the inherent subjectivity of user summaries, and human evaluation which is often costly and time-consuming. To address these challenges, we introduce \UserSumBench, a benchmark framework designed to facilitate iterative development of LLM-based summarization approaches. This framework offers two key components: (1) A reference-free summary quality metric. We show that this metric is effective and aligned with human preferences across three diverse datasets (MovieLens, Yelp and Amazon Review). (2) A novel robust summarization method that leverages time-hierarchical summarizer and self-critique verifier to produce high-quality summaries while eliminating hallucination. This method serves as a strong baseline for further innovation in summarization techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.16966v2-abstract-full').style.display = 'none'; document.getElementById('2408.16966v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.16717">arXiv:2408.16717</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.16717">pdf</a>, <a href="https://arxiv.org/format/2408.16717">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A GREAT Architecture for Edge-Based Graph Problems Like TSP
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lischka%2C+A">Attila Lischka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiaming Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chehreghani%2C+M+H">Morteza Haghir Chehreghani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kulcs%C3%A1r%2C+B">Balzs Kulcsr</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.16717v1-abstract-short" style="display: inline;">
        In the last years, many neural network-based approaches have been proposed to tackle combinatorial optimization problems such as routing problems. Many of these approaches are based on graph neural networks (GNNs) or related transformers, operating on the Euclidean coordinates representing the routing problems. However, GNNs are inherently not well suited to operate on dense graphs, such as in rou&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.16717v1-abstract-full').style.display = 'inline'; document.getElementById('2408.16717v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.16717v1-abstract-full" style="display: none;">
        In the last years, many neural network-based approaches have been proposed to tackle combinatorial optimization problems such as routing problems. Many of these approaches are based on graph neural networks (GNNs) or related transformers, operating on the Euclidean coordinates representing the routing problems. However, GNNs are inherently not well suited to operate on dense graphs, such as in routing problems. Furthermore, models operating on Euclidean coordinates cannot be applied to non-Euclidean versions of routing problems that are often found in real-world settings. To overcome these limitations, we propose a novel GNN-related edge-based neural model called Graph Edge Attention Network (GREAT). We evaluate the performance of GREAT in the edge-classification task to predict optimal edges in the Traveling Salesman Problem (TSP). We can use such a trained GREAT model to produce sparse TSP graph instances, keeping only the edges GREAT finds promising. Compared to other, non-learning-based methods to sparsify TSP graphs, GREAT can produce very sparse graphs while keeping most of the optimal edges. Furthermore, we build a reinforcement learning-based GREAT framework which we apply to Euclidean and non-Euclidean asymmetric TSP. This framework achieves state-of-the-art results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.16717v1-abstract-full').style.display = 'none'; document.getElementById('2408.16717v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.16518">arXiv:2408.16518</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.16518">pdf</a>, <a href="https://arxiv.org/format/2408.16518">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CNIMA: A Universal Evaluation Framework and Automated Approach for Assessing Second Language Dialogues
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+R">Rena Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingxuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roever%2C+C">Carsten Roever</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xuetong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+L">Long Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lau%2C+J+H">Jey Han Lau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.16518v1-abstract-short" style="display: inline;">
        We develop CNIMA (Chinese Non-Native Interactivity Measurement and Automation), a Chinese-as-a-second-language labelled dataset with 10K dialogues. We annotate CNIMA using an evaluation framework -- originally introduced for English-as-a-second-language dialogues -- that assesses micro-level features (e.g.\ backchannels) and macro-level interactivity labels (e.g.\ topic management) and test the fr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.16518v1-abstract-full').style.display = 'inline'; document.getElementById('2408.16518v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.16518v1-abstract-full" style="display: none;">
        We develop CNIMA (Chinese Non-Native Interactivity Measurement and Automation), a Chinese-as-a-second-language labelled dataset with 10K dialogues. We annotate CNIMA using an evaluation framework -- originally introduced for English-as-a-second-language dialogues -- that assesses micro-level features (e.g.\ backchannels) and macro-level interactivity labels (e.g.\ topic management) and test the framework&#39;s transferability from English to Chinese. We found the framework robust across languages and revealed universal and language-specific relationships between micro-level and macro-level features. Next, we propose an approach to automate the evaluation and find strong performance, creating a new tool for automated second language assessment. Our system can be adapted to other languages easily as it uses large language models and as such does not require large-scale annotated training data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.16518v1-abstract-full').style.display = 'none'; document.getElementById('2408.16518v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>