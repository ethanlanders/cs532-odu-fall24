cs.LG cs.DC
Buffer-based Gradient Projection for Continual Federated Learning
Authors: Shenghong Dai , Jy-yong Sohn , Yicong Chen , S M Iftekharul Alam , Ravikumar Balakrishnan , Suman Banerjee , Nageen Himayat , Kangwook Lee
Abstract: Continual Federated Learning (CFL) is essential for enabling real-world applications where multiple decentralized clients adaptively learn from continuous data streams. A significant challenge in CFL is mitigating catastrophic forgetting, where models lose previously acquired knowledge when learning new information. Existing approaches often face difficulties due to the constraints of device stora… ▽ More Continual Federated Learning (CFL) is essential for enabling real-world applications where multiple decentralized clients adaptively learn from continuous data streams. A significant challenge in CFL is mitigating catastrophic forgetting, where models lose previously acquired knowledge when learning new information. Existing approaches often face difficulties due to the constraints of device storage capacities and the heterogeneous nature of data distributions among clients. While some CFL algorithms have addressed these challenges, they frequently rely on unrealistic assumptions about the availability of task boundaries (i.e., knowing when new tasks begin). To address these limitations, we introduce Fed-A-GEM, a federated adaptation of the A-GEM method (Chaudhry et al., 2019), which employs a buffer-based gradient projection approach. Fed-A-GEM alleviates catastrophic forgetting by leveraging local buffer samples and aggregated buffer gradients, thus preserving knowledge across multiple clients. Our method is combined with existing CFL techniques, enhancing their performance in the CFL context. Our experiments on standard benchmarks show consistent performance improvements across diverse scenarios. For example, in a task-incremental learning scenario using the CIFAR-100 dataset, our method can increase the accuracy by up to 27%. Our code is available at https://github.com/shenghongdai/Fed-A-GEM. △ Less
Submitted 2 September, 2024; originally announced September 2024.
Comments: A preliminary version of this work was presented at the Federated Learning Systems (FLSys) Workshop @ Sixth Conference on Machine Learning and Systems, June 2023
cs.CL
BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment Analysis
Authors: Sadia Alam , Md Farhan Ishmam , Navid Hasin Alvee , Md Shahnewaz Siddique , Md Azam Hossain , Abu Raihan Mostofa Kamal
Abstract: The widespread availability of code-mixed data can provide valuable insights into low-resource languages like Bengali, which have limited datasets. Sentiment analysis has been a fundamental text classification task across several languages for code-mixed data. However, there has yet to be a large-scale and diverse sentiment analysis dataset on code-mixed Bengali. We address this limitation by intr… ▽ More The widespread availability of code-mixed data can provide valuable insights into low-resource languages like Bengali, which have limited datasets. Sentiment analysis has been a fundamental text classification task across several languages for code-mixed data. However, there has yet to be a large-scale and diverse sentiment analysis dataset on code-mixed Bengali. We address this limitation by introducing BnSentMix, a sentiment analysis dataset on code-mixed Bengali consisting of 20,000 samples with $4$ sentiment labels from Facebook, YouTube, and e-commerce sites. We ensure diversity in data sources to replicate realistic code-mixed scenarios. Additionally, we propose $14$ baseline methods including novel transformer encoders further pre-trained on code-mixed Bengali-English, achieving an overall accuracy of $69.8\%$ and an F1 score of $69.1\%$ on sentiment classification tasks. Detailed analyses reveal variations in performance across different sentiment labels and text types, highlighting areas for future improvement. △ Less
Submitted 16 August, 2024; originally announced August 2024.
cond-mat.mes-hall cs.AR
Harnessing Ferro-Valleytricity in Penta-Layer Rhombohedral Graphene for Memory and Compute
Authors: Md Mazharul Islam , Shamiul Alam , Md Rahatul Islam Udoy , Md Shafayat Hossain , Kathleen E Hamilton , Ahmedullah Aziz
Abstract: Two-dimensional materials with multiple degrees of freedom, including spin, valleys, and orbitals, open up an exciting avenue for engineering multifunctional devices. Beyond spintronics, these degrees of freedom can lead to novel quantum effects such as valley-dependent Hall effects and orbital magnetism, which could revolutionize next-generation electronics. However, achieving independent control… ▽ More Two-dimensional materials with multiple degrees of freedom, including spin, valleys, and orbitals, open up an exciting avenue for engineering multifunctional devices. Beyond spintronics, these degrees of freedom can lead to novel quantum effects such as valley-dependent Hall effects and orbital magnetism, which could revolutionize next-generation electronics. However, achieving independent control over valley polarization and orbital magnetism has been a challenge due to the need for large electric fields. A recent breakthrough involving penta-layer rhombohedral graphene has demonstrated the ability to individually manipulate anomalous Hall signals and orbital magnetic hysteresis, forming what is known as a valley-magnetic quartet. Here, we leverage the electrically tunable Ferro-valleytricity of penta-layer rhombohedral graphene to develop non-volatile memory and in-memory computation applications. We propose an architecture for a dense, scalable, and selector-less non-volatile memory array that harnesses the electrically tunable ferro-valleytricity. In our designed array architecture, non-destructive read and write operations are conducted by sensing the valley state through two different pairs of terminals, allowing for independent optimization of read/write peripheral circuits. The power consumption of our PRG-based array is remarkably low, with only ~ 6 nW required per write operation and ~ 2.3 nW per read operation per cell. This consumption is orders of magnitude lower than that of the majority of state-of-the-art cryogenic memories. Additionally, we engineer in-memory computation by implementing majority logic operations within our proposed non-volatile memory array without modifying the peripheral circuitry. Our framework presents a promising pathway toward achieving ultra-dense cryogenic memory and in-memory computation capabilities. △ Less
Submitted 2 August, 2024; originally announced August 2024.
arXiv:2407.19528 [ pdf , other ]
cs.CL
Motamot: A Dataset for Revealing the Supremacy of Large Language Models over Transformer Models in Bengali Political Sentiment Analysis
Authors: Fatema Tuj Johora Faria , Mukaffi Bin Moin , Rabeya Islam Mumu , Md Mahabubul Alam Abir , Abrar Nawar Alfy , Mohammad Shafiul Alam
Abstract: Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. Analyzing political sentiment is critical for understanding the complexities of public opinion processes, especially during election seasons. It gives significant information on voter preferences, attitudes, and current trends. In this study, we investigate political sentiment… ▽ More Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. Analyzing political sentiment is critical for understanding the complexities of public opinion processes, especially during election seasons. It gives significant information on voter preferences, attitudes, and current trends. In this study, we investigate political sentiment analysis during Bangladeshi elections, specifically examining how effectively Pre-trained Language Models (PLMs) and Large Language Models (LLMs) capture complex sentiment characteristics. Our study centers on the creation of the "Motamot" dataset, comprising 7,058 instances annotated with positive and negative sentiments, sourced from diverse online newspaper portals, forming a comprehensive resource for political sentiment analysis. We meticulously evaluate the performance of various PLMs including BanglaBERT, Bangla BERT Base, XLM-RoBERTa, mBERT, and sahajBERT, alongside LLMs such as Gemini 1.5 Pro and GPT 3.5 Turbo. Moreover, we explore zero-shot and few-shot learning strategies to enhance our understanding of political sentiment analysis methodologies. Our findings underscore BanglaBERT's commendable accuracy of 88.10% among PLMs. However, the exploration into LLMs reveals even more promising results. Through the adept application of Few-Shot learning techniques, Gemini 1.5 Pro achieves an impressive accuracy of 96.33%, surpassing the remarkable performance of GPT 3.5 Turbo, which stands at 94%. This underscores Gemini 1.5 Pro's status as the superior performer in this comparison. △ Less
Submitted 28 July, 2024; originally announced July 2024.
