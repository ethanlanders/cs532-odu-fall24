<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 2,524 results for author: <span class="mathjax">Wu, J</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Wu, J">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Wu%2C+J&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Wu, J">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.02352">arXiv:2412.02352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.02352">pdf</a>, <a href="https://arxiv.org/format/2412.02352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LoRA Diffusion: Zero-Shot LoRA Synthesis for Diffusion Model Personalization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Smith%2C+E">Ethan Smith</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seid%2C+R">Rami Seid</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hojel%2C+A">Alberto Hojel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mishra%2C+P">Paramita Mishra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianbo Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.02352v1-abstract-short" style="display: inline;">
        Low-Rank Adaptation (LoRA) and other parameter-efficient fine-tuning (PEFT) methods provide low-memory, storage-efficient solutions for personalizing text-to-image models. However, these methods offer little to no improvement in wall-clock training time or the number of steps needed for convergence compared to full model fine-tuning. While PEFT methods assume that shifts in generated distributions&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02352v1-abstract-full').style.display = 'inline'; document.getElementById('2412.02352v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.02352v1-abstract-full" style="display: none;">
        Low-Rank Adaptation (LoRA) and other parameter-efficient fine-tuning (PEFT) methods provide low-memory, storage-efficient solutions for personalizing text-to-image models. However, these methods offer little to no improvement in wall-clock training time or the number of steps needed for convergence compared to full model fine-tuning. While PEFT methods assume that shifts in generated distributions (from base to fine-tuned models) can be effectively modeled through weight changes in a low-rank subspace, they fail to leverage knowledge of common use cases, which typically focus on capturing specific styles or identities. Observing that desired outputs often comprise only a small subset of the possible domain covered by LoRA training, we propose reducing the search space by incorporating a prior over regions of interest. We demonstrate that training a hypernetwork model to generate LoRA weights can achieve competitive quality for specific domains while enabling near-instantaneous conditioning on user input, in contrast to traditional training methods that require thousands of steps.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02352v1-abstract-full').style.display = 'none'; document.getElementById('2412.02352v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.02193">arXiv:2412.02193</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.02193">pdf</a>, <a href="https://arxiv.org/format/2412.02193">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+F">Fan-Yun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+W">Weiyu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+S">Siyi Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lim%2C+D">Dylan Lim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bhat%2C+G">Goutam Bhat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tombari%2C+F">Federico Tombari</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+M">Manling Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Haber%2C+N">Nick Haber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.02193v1-abstract-short" style="display: inline;">
        Open-universe 3D layout generation arranges unlabeled 3D assets conditioned on language instruction. Large language models (LLMs) struggle with generating physically plausible 3D scenes and adherence to input instructions, particularly in cluttered scenes. We introduce LayoutVLM, a framework and scene layout representation that exploits the semantic knowledge of Vision-Language Models (VLMs) and s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02193v1-abstract-full').style.display = 'inline'; document.getElementById('2412.02193v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.02193v1-abstract-full" style="display: none;">
        Open-universe 3D layout generation arranges unlabeled 3D assets conditioned on language instruction. Large language models (LLMs) struggle with generating physically plausible 3D scenes and adherence to input instructions, particularly in cluttered scenes. We introduce LayoutVLM, a framework and scene layout representation that exploits the semantic knowledge of Vision-Language Models (VLMs) and supports differentiable optimization to ensure physical plausibility. LayoutVLM employs VLMs to generate two mutually reinforcing representations from visually marked images, and a self-consistent decoding process to improve VLMs spatial planning. Our experiments show that LayoutVLM addresses the limitations of existing LLM and constraint-based approaches, producing physically plausible 3D layouts better aligned with the semantic intent of input language instructions. We also demonstrate that fine-tuning VLMs with the proposed scene layout representation extracted from existing scene datasets can improve performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02193v1-abstract-full').style.display = 'none'; document.getElementById('2412.02193v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">project website: https://ai.stanford.edu/~sunfanyun/layoutvlm/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.02177">arXiv:2412.02177</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.02177">pdf</a>, <a href="https://arxiv.org/format/2412.02177">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Anatomically-Grounded Fact Checking of Automated Chest X-ray Reports
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mahmood%2C+R">R. Mahmood</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wong%2C+K+C+L">K. C. L. Wong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reyes%2C+D+M">D. M. Reyes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=D%27Souza%2C+N">N. D&#39;Souza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+L">L. Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">J. Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaviani%2C+P">P. Kaviani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kalra%2C+M">M. Kalra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">G. Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+P">P. Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Syeda-Mahmood%2C+T">T. Syeda-Mahmood</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.02177v1-abstract-short" style="display: inline;">
        With the emergence of large-scale vision-language models, realistic radiology reports may be generated using only medical images as input guided by simple prompts. However, their practical utility has been limited due to the factual errors in their description of findings. In this paper, we propose a novel model for explainable fact-checking that identifies errors in findings and their locations i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02177v1-abstract-full').style.display = 'inline'; document.getElementById('2412.02177v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.02177v1-abstract-full" style="display: none;">
        With the emergence of large-scale vision-language models, realistic radiology reports may be generated using only medical images as input guided by simple prompts. However, their practical utility has been limited due to the factual errors in their description of findings. In this paper, we propose a novel model for explainable fact-checking that identifies errors in findings and their locations indicated through the reports. Specifically, we analyze the types of errors made by automated reporting methods and derive a new synthetic dataset of images paired with real and fake descriptions of findings and their locations from a ground truth dataset. A new multi-label cross-modal contrastive regression network is then trained on this datsaset. We evaluate the resulting fact-checking model and its utility in correcting reports generated by several SOTA automated reporting tools on a variety of benchmark datasets with results pointing to over 40\% improvement in report quality through such error detection and correction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02177v1-abstract-full').style.display = 'none'; document.getElementById('2412.02177v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          RPI12
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.02142">arXiv:2412.02142</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.02142">pdf</a>, <a href="https://arxiv.org/format/2412.02142">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalized Multimodal Large Language Models: A Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junda Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+H">Hanjia Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+Y">Yu Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhehao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barrow%2C+J">Joe Barrow</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+I">Ishita Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mirtaheri%2C+M">Mehrnoosh Mirtaheri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hongjie Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rossi%2C+R+A">Ryan A. Rossi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dernoncourt%2C+F">Franck Dernoncourt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+T">Tong Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+R">Ruiyi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+J">Jiuxiang Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ahmed%2C+N+K">Nesreen K. Ahmed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xiang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deilamsalehy%2C+H">Hanieh Deilamsalehy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+N">Namyong Park</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Sungchul Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+H">Huanrui Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mitra%2C+S">Subrata Mitra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Z">Zhengmian Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lipka%2C+N">Nedim Lipka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+D">Dang Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yue Zhao</a>
      , et al. (2 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.02142v1-abstract-short" style="display: inline;">
        Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02142v1-abstract-full').style.display = 'inline'; document.getElementById('2412.02142v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.02142v1-abstract-full" style="display: none;">
        Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02142v1-abstract-full').style.display = 'none'; document.getElementById('2412.02142v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.01525">arXiv:2412.01525</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.01525">pdf</a>, <a href="https://arxiv.org/format/2412.01525">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Take Your Steps: Hierarchically Efficient Pulmonary Disease Screening via CT Volume Compression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+Q">Qian Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Kai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+B">Bang Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zepeng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yixuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Q">Qiyuan Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jintai Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+H">Honghao Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hongxia Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.01525v2-abstract-short" style="display: inline;">
        Deep learning models are widely used to process Computed Tomography (CT) data in the automated screening of pulmonary diseases, significantly reducing the workload of physicians. However, the three-dimensional nature of CT volumes involves an excessive number of voxels, which significantly increases the complexity of model processing. Previous screening approaches often overlook this issue, which&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01525v2-abstract-full').style.display = 'inline'; document.getElementById('2412.01525v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.01525v2-abstract-full" style="display: none;">
        Deep learning models are widely used to process Computed Tomography (CT) data in the automated screening of pulmonary diseases, significantly reducing the workload of physicians. However, the three-dimensional nature of CT volumes involves an excessive number of voxels, which significantly increases the complexity of model processing. Previous screening approaches often overlook this issue, which undoubtedly reduces screening efficiency. Towards efficient and effective screening, we design a hierarchical approach to reduce the computational cost of pulmonary disease screening. The new approach re-organizes the screening workflows into three steps. First, we propose a Computed Tomography Volume Compression (CTVC) method to select a small slice subset that comprehensively represents the whole CT volume. Second, the selected CT slices are used to detect pulmonary diseases coarsely via a lightweight classification model. Third, an uncertainty measurement strategy is applied to identify samples with low diagnostic confidence, which are re-detected by radiologists. Experiments on two public pulmonary disease datasets demonstrate that our approach achieves comparable accuracy and recall while reducing the time by 50%-70% compared with the counterparts using full CT volumes. Besides, we also found that our approach outperforms previous cutting-edge CTVC methods in retaining important indications after compression.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01525v2-abstract-full').style.display = 'none'; document.getElementById('2412.01525v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 December, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 December, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under Review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.01317">arXiv:2412.01317</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.01317">pdf</a>, <a href="https://arxiv.org/format/2412.01317">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Seeds of the FUTURE Sprout from History: Fuzzing for Unveiling Vulnerabilities in Prospective Deep-Learning Libraries
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhiyuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingzheng Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ling%2C+X">Xiang Ling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+T">Tianyue Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rui%2C+Z">Zhiqing Rui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yanjun Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.01317v1-abstract-short" style="display: inline;">
        The widespread application of large language models (LLMs) underscores the importance of deep learning (DL) technologies that rely on foundational DL libraries such as PyTorch and TensorFlow. Despite their robust features, these libraries face challenges with scalability and adaptation to rapid advancements in the LLM community. In response, tech giants like Apple and Huawei are developing their o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01317v1-abstract-full').style.display = 'inline'; document.getElementById('2412.01317v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.01317v1-abstract-full" style="display: none;">
        The widespread application of large language models (LLMs) underscores the importance of deep learning (DL) technologies that rely on foundational DL libraries such as PyTorch and TensorFlow. Despite their robust features, these libraries face challenges with scalability and adaptation to rapid advancements in the LLM community. In response, tech giants like Apple and Huawei are developing their own DL libraries to enhance performance, increase scalability, and safeguard intellectual property. Ensuring the security of these libraries is crucial, with fuzzing being a vital solution. However, existing fuzzing frameworks struggle with target flexibility, effectively testing bug-prone API sequences, and leveraging the limited available information in new libraries. To address these limitations, we propose FUTURE, the first universal fuzzing framework tailored for newly introduced and prospective DL libraries. FUTURE leverages historical bug information from existing libraries and fine-tunes LLMs for specialized code generation. This strategy helps identify bugs in new libraries and uses insights from these libraries to enhance security in existing ones, creating a cycle from history to future and back. To evaluate FUTURE&#39;s effectiveness, we conduct comprehensive evaluations on three newly introduced DL libraries. Evaluation results demonstrate that FUTURE significantly outperforms existing fuzzers in bug detection, success rate of bug reproduction, validity rate of code generation, and API coverage. Notably, FUTURE has detected 148 bugs across 452 targeted APIs, including 142 previously unknown bugs. Among these, 10 have been assigned CVE IDs. Additionally, FUTURE detects 7 bugs in PyTorch, demonstrating its ability to enhance security in existing libraries in reverse.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01317v1-abstract-full').style.display = 'none'; document.getElementById('2412.01317v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted by 47th International Conference on Software Engineering (ICSE 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.01051">arXiv:2412.01051</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.01051">pdf</a>, <a href="https://arxiv.org/format/2412.01051">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Efficient Unsupervised Framework for Convex Quadratic Programs via Deep Unrolling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+L">Linxin Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bingheng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+T">Tian Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianghua Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+A">Akang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yuyi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+J">Jiliang Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+R">Ruoyu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+X">Xiaodong Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.01051v1-abstract-short" style="display: inline;">
        Quadratic programs (QPs) arise in various domains such as machine learning, finance, and control. Recently, learning-enhanced primal-dual hybrid gradient (PDHG) methods have shown great potential in addressing large-scale linear programs; however, this approach has not been extended to QPs. In this work, we focus on unrolling &#34;PDQP&#34;, a PDHG algorithm specialized for convex QPs. Specifically, we pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01051v1-abstract-full').style.display = 'inline'; document.getElementById('2412.01051v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.01051v1-abstract-full" style="display: none;">
        Quadratic programs (QPs) arise in various domains such as machine learning, finance, and control. Recently, learning-enhanced primal-dual hybrid gradient (PDHG) methods have shown great potential in addressing large-scale linear programs; however, this approach has not been extended to QPs. In this work, we focus on unrolling &#34;PDQP&#34;, a PDHG algorithm specialized for convex QPs. Specifically, we propose a neural network model called &#34;PDQP-net&#34; to learn optimal QP solutions. Theoretically, we demonstrate that a PDQP-net of polynomial size can align with the PDQP algorithm, returning optimal primal-dual solution pairs. We propose an unsupervised method that incorporates KKT conditions into the loss function. Unlike the standard learning-to-optimize framework that requires optimization solutions generated by solvers, our unsupervised method adjusts the network weights directly from the evaluation of the primal-dual gap. This method has two benefits over supervised learning: first, it helps generate better primal-dual gap since the primal-dual gap is in the objective function; second, it does not require solvers. We show that PDQP-net trained in this unsupervised manner can effectively approximate optimal QP solutions. Extensive numerical experiments confirm our findings, indicating that using PDQP-net predictions to warm-start PDQP can achieve up to 45% acceleration on QP instances. Moreover, it achieves 14% to 31% acceleration on out-of-distribution instances.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01051v1-abstract-full').style.display = 'none'; document.getElementById('2412.01051v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.01031">arXiv:2412.01031</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.01031">pdf</a>, <a href="https://arxiv.org/format/2412.01031">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mahmood%2C+R">Razi Mahmood</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+P">Pingkun Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reyes%2C+D+M">Diego Machado Reyes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Ge Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kalra%2C+M+K">Mannudeep K. Kalra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaviani%2C+P">Parisa Kaviani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J+T">Joy T. Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Syeda-Mahmood%2C+T">Tanveer Syeda-Mahmood</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.01031v1-abstract-short" style="display: inline;">
        Several evaluation metrics have been developed recently to automatically assess the quality of generative AI reports for chest radiographs based only on textual information using lexical, semantic, or clinical named entity recognition methods. In this paper, we develop a new method of report quality evaluation by first extracting fine-grained finding patterns capturing the location, laterality, an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01031v1-abstract-full').style.display = 'inline'; document.getElementById('2412.01031v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.01031v1-abstract-full" style="display: none;">
        Several evaluation metrics have been developed recently to automatically assess the quality of generative AI reports for chest radiographs based only on textual information using lexical, semantic, or clinical named entity recognition methods. In this paper, we develop a new method of report quality evaluation by first extracting fine-grained finding patterns capturing the location, laterality, and severity of a large number of clinical findings. We then performed phrasal grounding to localize their associated anatomical regions on chest radiograph images. The textual and visual measures are then combined to rate the quality of the generated reports. We present results that compare this evaluation metric with other textual metrics on a gold standard dataset derived from the MIMIC collection and show its robustness and sensitivity to factual errors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.01031v1-abstract-full').style.display = 'none'; document.getElementById('2412.01031v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.00797">arXiv:2412.00797</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.00797">pdf</a>, <a href="https://arxiv.org/format/2412.00797">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Online Poisoning Attack Against Reinforcement Learning under Black-box Environments
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jianhui Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+B">Bokang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junfeng Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.00797v1-abstract-short" style="display: inline;">
        This paper proposes an online environment poisoning algorithm tailored for reinforcement learning agents operating in a black-box setting, where an adversary deliberately manipulates training data to lead the agent toward a mischievous policy. In contrast to prior studies that primarily investigate white-box settings, we focus on a scenario characterized by \textit{unknown} environment dynamics to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00797v1-abstract-full').style.display = 'inline'; document.getElementById('2412.00797v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.00797v1-abstract-full" style="display: none;">
        This paper proposes an online environment poisoning algorithm tailored for reinforcement learning agents operating in a black-box setting, where an adversary deliberately manipulates training data to lead the agent toward a mischievous policy. In contrast to prior studies that primarily investigate white-box settings, we focus on a scenario characterized by \textit{unknown} environment dynamics to the attacker and a \textit{flexible} reinforcement learning algorithm employed by the targeted agent. We first propose an attack scheme that is capable of poisoning the reward functions and state transitions. The poisoning task is formalized as a constrained optimization problem, following the framework of \cite{ma2019policy}. Given the transition probabilities are unknown to the attacker in a black-box environment, we apply a stochastic gradient descent algorithm, where the exact gradients are approximated using sample-based estimates. A penalty-based method along with a bilevel reformulation is then employed to transform the problem into an unconstrained counterpart and to circumvent the double-sampling issue. The algorithm&#39;s effectiveness is validated through a maze environment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00797v1-abstract-full').style.display = 'none'; document.getElementById('2412.00797v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.00383">arXiv:2412.00383</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.00383">pdf</a>, <a href="https://arxiv.org/format/2412.00383">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unified Parameter-Efficient Unlearning for LLMs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+C">Chenlu Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiancan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+Y">Yancheng Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+J">Jinda Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Kai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+A">Alex Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+X">Xiangnan He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.00383v1-abstract-short" style="display: inline;">
        The advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like LoRA, has become a prevalent practice due to its efficiency. However, this raises significant privac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00383v1-abstract-full').style.display = 'inline'; document.getElementById('2412.00383v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.00383v1-abstract-full" style="display: none;">
        The advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like LoRA, has become a prevalent practice due to its efficiency. However, this raises significant privacy and security concerns, as models may inadvertently retain and disseminate sensitive or undesirable information. To address these issues, we introduce a novel instance-wise unlearning framework, LLMEraser, which systematically categorizes unlearning tasks and applies precise parameter adjustments using influence functions. Unlike traditional unlearning techniques that are often limited in scope and require extensive retraining, LLMEraser is designed to handle a broad spectrum of unlearning tasks without compromising model performance. Extensive experiments on benchmark datasets demonstrate that LLMEraser excels in efficiently managing various unlearning scenarios while maintaining the overall integrity and efficacy of the models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00383v1-abstract-full').style.display = 'none'; document.getElementById('2412.00383v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.00291">arXiv:2412.00291</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.00291">pdf</a>, <a href="https://arxiv.org/format/2412.00291">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TASE.2024.3429280">10.1109/TASE.2024.3429280 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Real-Time Metric-Semantic Mapping for Autonomous Navigation in Outdoor Environments
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jiao%2C+J">Jianhao Jiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geng%2C+R">Ruoyu Geng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yuanhang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xin%2C+R">Ren Xin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+B">Bowen Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Lujia Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+M">Ming Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+R">Rui Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kanoulas%2C+D">Dimitrios Kanoulas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.00291v1-abstract-short" style="display: inline;">
        The creation of a metric-semantic map, which encodes human-prior knowledge, represents a high-level abstraction of environments. However, constructing such a map poses challenges related to the fusion of multi-modal sensor data, the attainment of real-time mapping performance, and the preservation of structural and semantic information consistency. In this paper, we introduce an online metric-sema&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00291v1-abstract-full').style.display = 'inline'; document.getElementById('2412.00291v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.00291v1-abstract-full" style="display: none;">
        The creation of a metric-semantic map, which encodes human-prior knowledge, represents a high-level abstraction of environments. However, constructing such a map poses challenges related to the fusion of multi-modal sensor data, the attainment of real-time mapping performance, and the preservation of structural and semantic information consistency. In this paper, we introduce an online metric-semantic mapping system that utilizes LiDAR-Visual-Inertial sensing to generate a global metric-semantic mesh map of large-scale outdoor environments. Leveraging GPU acceleration, our mapping process achieves exceptional speed, with frame processing taking less than 7ms, regardless of scenario scale. Furthermore, we seamlessly integrate the resultant map into a real-world navigation system, enabling metric-semantic-based terrain assessment and autonomous point-to-point navigation within a campus environment. Through extensive experiments conducted on both publicly available and self-collected datasets comprising 24 sequences, we demonstrate the effectiveness of our mapping and navigation methodologies. Code has been publicly released: https://github.com/gogojjh/cobra
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00291v1-abstract-full').style.display = 'none'; document.getElementById('2412.00291v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 9 figures, accepted to IEEE Transactions on Automation Science and Engineering</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.19654">arXiv:2411.19654</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.19654">pdf</a>, <a href="https://arxiv.org/format/2411.19654">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TexGaussian: Generating High-quality PBR Material via Octree-based 3D Gaussian Splatting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiong%2C+B">Bojun Xiong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jialun Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+J">Jiakui Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chenming Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinbo Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xing Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+C">Chen Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+E">Errui Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lian%2C+Z">Zhouhui Lian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.19654v1-abstract-short" style="display: inline;">
        Physically Based Rendering (PBR) materials play a crucial role in modern graphics, enabling photorealistic rendering across diverse environment maps. Developing an effective and efficient algorithm that is capable of automatically generating high-quality PBR materials rather than RGB texture for 3D meshes can significantly streamline the 3D content creation. Most existing methods leverage pre-trai&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.19654v1-abstract-full').style.display = 'inline'; document.getElementById('2411.19654v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.19654v1-abstract-full" style="display: none;">
        Physically Based Rendering (PBR) materials play a crucial role in modern graphics, enabling photorealistic rendering across diverse environment maps. Developing an effective and efficient algorithm that is capable of automatically generating high-quality PBR materials rather than RGB texture for 3D meshes can significantly streamline the 3D content creation. Most existing methods leverage pre-trained 2D diffusion models for multi-view image synthesis, which often leads to severe inconsistency between the generated textures and input 3D meshes. This paper presents TexGaussian, a novel method that uses octant-aligned 3D Gaussian Splatting for rapid PBR material generation. Specifically, we place each 3D Gaussian on the finest leaf node of the octree built from the input 3D mesh to render the multiview images not only for the albedo map but also for roughness and metallic. Moreover, our model is trained in a regression manner instead of diffusion denoising, capable of generating the PBR material for a 3D mesh in a single feed-forward process. Extensive experiments on publicly available benchmarks demonstrate that our method synthesizes more visually pleasing PBR materials and runs faster than previous methods in both unconditional and text-conditional scenarios, which exhibit better consistency with the given geometry. Our code and trained models are available at https://3d-aigc.github.io/TexGaussian.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.19654v1-abstract-full').style.display = 'none'; document.getElementById('2411.19654v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical Report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.18808">arXiv:2411.18808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.18808">pdf</a>, <a href="https://arxiv.org/format/2411.18808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lifting Motion to the 3D World via 2D Diffusion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jiaman Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C+K">C. Karen Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.18808v1-abstract-short" style="display: inline;">
        Estimating 3D motion from 2D observations is a long-standing research challenge. Prior work typically requires training on datasets containing ground truth 3D motions, limiting their applicability to activities well-represented in existing motion capture data. This dependency particularly hinders generalization to out-of-distribution scenarios or subjects where collecting 3D ground truth is challe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18808v1-abstract-full').style.display = 'inline'; document.getElementById('2411.18808v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.18808v1-abstract-full" style="display: none;">
        Estimating 3D motion from 2D observations is a long-standing research challenge. Prior work typically requires training on datasets containing ground truth 3D motions, limiting their applicability to activities well-represented in existing motion capture data. This dependency particularly hinders generalization to out-of-distribution scenarios or subjects where collecting 3D ground truth is challenging, such as complex athletic movements or animal motion. We introduce MVLift, a novel approach to predict global 3D motion -- including both joint rotations and root trajectories in the world coordinate system -- using only 2D pose sequences for training. Our multi-stage framework leverages 2D motion diffusion models to progressively generate consistent 2D pose sequences across multiple views, a key step in recovering accurate global 3D motion. MVLift generalizes across various domains, including human poses, human-object interactions, and animal poses. Despite not requiring 3D supervision, it outperforms prior work on five datasets, including those methods that require 3D supervision.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18808v1-abstract-full').style.display = 'none'; document.getElementById('2411.18808v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">project page: https://lijiaman.github.io/projects/mvlift/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.18616">arXiv:2411.18616</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.18616">pdf</a>, <a href="https://arxiv.org/format/2411.18616">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diffusion Self-Distillation for Zero-Shot Customized Image Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+S">Shengqu Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chan%2C+E">Eric Chan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yunzhi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guibas%2C+L">Leonidas Guibas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wetzstein%2C+G">Gordon Wetzstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.18616v1-abstract-short" style="display: inline;">
        Text-to-image diffusion models produce impressive results but are frustrating tools for artists who desire fine-grained control. For example, a common use case is to create images of a specific instance in novel contexts, i.e., &#34;identity-preserving generation&#34;. This setting, along with many other tasks (e.g., relighting), is a natural fit for image+text-conditional generative models. However, ther&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18616v1-abstract-full').style.display = 'inline'; document.getElementById('2411.18616v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.18616v1-abstract-full" style="display: none;">
        Text-to-image diffusion models produce impressive results but are frustrating tools for artists who desire fine-grained control. For example, a common use case is to create images of a specific instance in novel contexts, i.e., &#34;identity-preserving generation&#34;. This setting, along with many other tasks (e.g., relighting), is a natural fit for image+text-conditional generative models. However, there is insufficient high-quality paired data to train such a model directly. We propose Diffusion Self-Distillation, a method for using a pre-trained text-to-image model to generate its own dataset for text-conditioned image-to-image tasks. We first leverage a text-to-image diffusion model&#39;s in-context generation ability to create grids of images and curate a large paired dataset with the help of a Visual-Language Model. We then fine-tune the text-to-image model into a text+image-to-image model using the curated paired dataset. We demonstrate that Diffusion Self-Distillation outperforms existing zero-shot methods and is competitive with per-instance tuning techniques on a wide range of identity-preservation generation tasks, without requiring test-time optimization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18616v1-abstract-full').style.display = 'none'; document.getElementById('2411.18616v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://primecai.github.io/dsd/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.18478">arXiv:2411.18478</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.18478">pdf</a>, <a href="https://arxiv.org/format/2411.18478">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinyang Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+M">Mingkuan Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shuai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Che%2C+F">Feihu Che</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Z">Zengqi Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tao%2C+J">Jianhua Tao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.18478v1-abstract-short" style="display: inline;">
        In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18478v1-abstract-full').style.display = 'inline'; document.getElementById('2411.18478v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.18478v1-abstract-full" style="display: none;">
        In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a \textbf{Hi}gh-level \textbf{A}utomated \textbf{R}easoning paradigm in \textbf{ICL} that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL&#39;s effectiveness, achieving state-of-the-art accuracy (79.6$\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6$\%$) and Claude 3.5 (71.1$\%$).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18478v1-abstract-full').style.display = 'none'; document.getElementById('2411.18478v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.18203">arXiv:2411.18203</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.18203">pdf</a>, <a href="https://arxiv.org/format/2411.18203">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+D">Di Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Junxian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+J">Jingdi Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xunzhi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yujie Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Zonglin Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jiatong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Weida Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+S">Suorong Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianbo Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+P">Peng Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+W">Wanli Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+D">Dongzhan Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.18203v2-abstract-short" style="display: inline;">
        Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18203v2-abstract-full').style.display = 'inline'; document.getElementById('2411.18203v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.18203v2-abstract-full" style="display: none;">
        Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner&#39;s capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18203v2-abstract-full').style.display = 'none'; document.getElementById('2411.18203v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 December, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.17928">arXiv:2411.17928</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.17928">pdf</a>, <a href="https://arxiv.org/format/2411.17928">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MapEval: Towards Unified, Robust and Efficient SLAM Map Evaluation Framework
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+X">Xiangcheng Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+M">Mingkai Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+H">Hongyu Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Y">Yi Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+B">Binqian Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Wei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+W">Wei He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+P">Ping Tan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.17928v1-abstract-short" style="display: inline;">
        Evaluating massive-scale point cloud maps in Simultaneous Localization and Mapping (SLAM) remains challenging, primarily due to the absence of unified, robust and efficient evaluation frameworks. We present MapEval, an open-source framework for comprehensive quality assessment of point cloud maps, specifically addressing SLAM scenarios where ground truth map is inherently sparse compared to the ma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17928v1-abstract-full').style.display = 'inline'; document.getElementById('2411.17928v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.17928v1-abstract-full" style="display: none;">
        Evaluating massive-scale point cloud maps in Simultaneous Localization and Mapping (SLAM) remains challenging, primarily due to the absence of unified, robust and efficient evaluation frameworks. We present MapEval, an open-source framework for comprehensive quality assessment of point cloud maps, specifically addressing SLAM scenarios where ground truth map is inherently sparse compared to the mapped environment. Through systematic analysis of existing evaluation metrics in SLAM applications, we identify their fundamental limitations and establish clear guidelines for consistent map quality assessment. Building upon these insights, we propose a novel Gaussian-approximated Wasserstein distance in voxelized space, enabling two complementary metrics under the same error standard: Voxelized Average Wasserstein Distance (AWD) for global geometric accuracy and Spatial Consistency Score (SCS) for local consistency evaluation. This theoretical foundation leads to significant improvements in both robustness against noise and computational efficiency compared to conventional metrics. Extensive experiments on both simulated and real-world datasets demonstrate that MapEval achieves at least \SI{100}{}-\SI{500}{} times faster while maintaining evaluation integrity. The MapEval library\footnote{\texttt{https://github.com/JokerJohn/Cloud\_Map\_Evaluation}} will be publicly available to promote standardized map evaluation practices in the robotics community.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17928v1-abstract-full').style.display = 'none'; document.getElementById('2411.17928v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 7 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.17711">arXiv:2411.17711</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.17711">pdf</a>, <a href="https://arxiv.org/format/2411.17711">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AnyECG: Foundational Models for Electrocardiogram Analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yue Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+X">Xu Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yaojun Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ying%2C+H">Haochao Ying</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rehg%2C+J+M">James Matthew Rehg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jimeng Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jintai Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.17711v1-abstract-short" style="display: inline;">
        Electrocardiogram (ECG), a non-invasive and affordable tool for cardiac monitoring, is highly sensitive in detecting acute heart attacks. However, due to the lengthy nature of ECG recordings, numerous machine learning methods have been developed for automated heart disease detection to reduce human workload. Despite these efforts, performance remains suboptimal. A key obstacle is the inherent comp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17711v1-abstract-full').style.display = 'inline'; document.getElementById('2411.17711v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.17711v1-abstract-full" style="display: none;">
        Electrocardiogram (ECG), a non-invasive and affordable tool for cardiac monitoring, is highly sensitive in detecting acute heart attacks. However, due to the lengthy nature of ECG recordings, numerous machine learning methods have been developed for automated heart disease detection to reduce human workload. Despite these efforts, performance remains suboptimal. A key obstacle is the inherent complexity of ECG data, which includes heterogeneity (e.g., varying sampling rates), high levels of noise, demographic-related pattern shifts, and intricate rhythm-event associations. To overcome these challenges, this paper introduces AnyECG, a foundational model designed to extract robust representations from any real-world ECG data. Specifically, a tailored ECG Tokenizer encodes each fixed-duration ECG fragment into a token and, guided by proxy tasks, converts noisy, continuous ECG features into discrete, compact, and clinically meaningful local rhythm codes. These codes encapsulate basic morphological, frequency, and demographic information (e.g., sex), effectively mitigating signal noise. We further pre-train the AnyECG to learn rhythmic pattern associations across ECG tokens, enabling the capture of cardiac event semantics. By being jointly pre-trained on diverse ECG data sources, AnyECG is capable of generalizing across a wide range of downstream tasks where ECG signals are recorded from various devices and scenarios. Experimental results in anomaly detection, arrhythmia detection, corrupted lead generation, and ultra-long ECG signal analysis demonstrate that AnyECG learns common ECG knowledge from data and significantly outperforms cutting-edge methods in each respective task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17711v1-abstract-full').style.display = 'none'; document.getElementById('2411.17711v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.17521">arXiv:2411.17521</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.17521">pdf</a>, <a href="https://arxiv.org/format/2411.17521">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BESTAnP: Bi-Step Efficient and Statistically Optimal Estimator for Acoustic-n-Point Problem
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+W">Wenliang Sheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Hongxu Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Lingpeng Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+G">Guangyang Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+Y">Yunling Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+Y">Yuze Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+C">Chao Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+Z">Ziyang Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junfeng Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.17521v1-abstract-short" style="display: inline;">
        We consider the acoustic-n-point (AnP) problem, which estimates the pose of a 2D forward-looking sonar (FLS) according to n 3D-2D point correspondences. We explore the nature of the measured partial spherical coordinates and reveal their inherent relationships to translation and orientation. Based on this, we propose a bi-step efficient and statistically optimal AnP (BESTAnP) algorithm that decoup&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17521v1-abstract-full').style.display = 'inline'; document.getElementById('2411.17521v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.17521v1-abstract-full" style="display: none;">
        We consider the acoustic-n-point (AnP) problem, which estimates the pose of a 2D forward-looking sonar (FLS) according to n 3D-2D point correspondences. We explore the nature of the measured partial spherical coordinates and reveal their inherent relationships to translation and orientation. Based on this, we propose a bi-step efficient and statistically optimal AnP (BESTAnP) algorithm that decouples the estimation of translation and orientation. Specifically, in the first step, the translation estimation is formulated as the range-based localization problem based on distance-only measurements. In the second step, the rotation is estimated via eigendecomposition based on azimuth-only measurements and the estimated translation. BESTAnP is the first AnP algorithm that gives a closed-form solution for the full six-degree pose. In addition, we conduct bias elimination for BESTAnP such that it owns the statistical property of consistency. Through simulation and real-world experiments, we demonstrate that compared with the state-of-the-art (SOTA) methods, BESTAnP is over ten times faster and features real-time capacity in resource-constrained platforms while exhibiting comparable accuracy. Moreover, for the first time, we embed BESTAnP into a sonar-based odometry which shows its effectiveness for trajectory estimation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17521v1-abstract-full').style.display = 'none'; document.getElementById('2411.17521v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.17361">arXiv:2411.17361</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.17361">pdf</a>, <a href="https://arxiv.org/format/2411.17361">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Robust Cross-Domain Recommendation with Joint Identifiability of User Preference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+J">Jing Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+Z">Zesheng Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+B">Bin Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Z">Zhiwen Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jia Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jian Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+M">Michael Sheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+L">Lina Yao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.17361v1-abstract-short" style="display: inline;">
        Recent cross-domain recommendation (CDR) studies assume that disentangled domain-shared and domain-specific user representations can mitigate domain gaps and facilitate effective knowledge transfer. However, achieving perfect disentanglement is challenging in practice, because user behaviors in CDR are highly complex, and the true underlying user preferences cannot be fully captured through observ&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17361v1-abstract-full').style.display = 'inline'; document.getElementById('2411.17361v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.17361v1-abstract-full" style="display: none;">
        Recent cross-domain recommendation (CDR) studies assume that disentangled domain-shared and domain-specific user representations can mitigate domain gaps and facilitate effective knowledge transfer. However, achieving perfect disentanglement is challenging in practice, because user behaviors in CDR are highly complex, and the true underlying user preferences cannot be fully captured through observed user-item interactions alone. Given this impracticability, we instead propose to model {\it joint identifiability} that establishes unique correspondence of user representations across domains, ensuring consistent preference modeling even when user behaviors exhibit shifts in different domains. To achieve this, we introduce a hierarchical user preference modeling framework that organizes user representations by the neural network encoder&#39;s depth, allowing separate treatment of shallow and deeper subspaces. In the shallow subspace, our framework models the interest centroids for each user within each domain, probabilistically determining the users&#39; interest belongings and selectively aligning these centroids across domains to ensure fine-grained consistency in domain-irrelevant features. For deeper subspace representations, we enforce joint identifiability by decomposing it into a shared cross-domain stable component and domain-variant components, linked by a bijective transformation for unique correspondence. Empirical studies on real-world CDR tasks with varying domain correlations demonstrate that our method consistently surpasses state-of-the-art, even with weakly correlated tasks, highlighting the importance of joint identifiability in achieving robust CDR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17361v1-abstract-full').style.display = 'none'; document.getElementById('2411.17361v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 6 figures, under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.16810">arXiv:2411.16810</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.16810">pdf</a>, <a href="https://arxiv.org/format/2411.16810">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discrete to Continuous: Generating Smooth Transition Poses from Sign Language Observation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+S">Shengeng Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+J">Jiayi He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+L">Lechao Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingjing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+D">Dan Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+R">Richang Hong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.16810v1-abstract-short" style="display: inline;">
        Generating continuous sign language videos from discrete segments is challenging due to the need for smooth transitions that preserve natural flow and meaning. Traditional approaches that simply concatenate isolated signs often result in abrupt transitions, disrupting video coherence. To address this, we propose a novel framework, Sign-D2C, that employs a conditional diffusion model to synthesize&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.16810v1-abstract-full').style.display = 'inline'; document.getElementById('2411.16810v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.16810v1-abstract-full" style="display: none;">
        Generating continuous sign language videos from discrete segments is challenging due to the need for smooth transitions that preserve natural flow and meaning. Traditional approaches that simply concatenate isolated signs often result in abrupt transitions, disrupting video coherence. To address this, we propose a novel framework, Sign-D2C, that employs a conditional diffusion model to synthesize contextually smooth transition frames, enabling the seamless construction of continuous sign language sequences. Our approach transforms the unsupervised problem of transition frame generation into a supervised training task by simulating the absence of transition frames through random masking of segments in long-duration sign videos. The model learns to predict these masked frames by denoising Gaussian noise, conditioned on the surrounding sign observations, allowing it to handle complex, unstructured transitions. During inference, we apply a linearly interpolating padding strategy that initializes missing frames through interpolation between boundary frames, providing a stable foundation for iterative refinement by the diffusion model. Extensive experiments on the PHOENIX14T, USTC-CSL100, and USTC-SLR500 datasets demonstrate the effectiveness of our method in producing continuous, natural sign language videos.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.16810v1-abstract-full').style.display = 'none'; document.getElementById('2411.16810v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.16729">arXiv:2411.16729</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.16729">pdf</a>, <a href="https://arxiv.org/format/2411.16729">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+F">Fan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+S">Siyuan Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+N">Naye Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhaohan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jingmei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+F">Fuxing Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+Z">Zhenqing Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+L">Leyao Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+L">Lanxin Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geng%2C+W">Weidong Geng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+X">Xin Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+B">Bozuo Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+D">Dingguo Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+H">Hui Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+B">Bin Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.16729v1-abstract-short" style="display: inline;">
        Speech-driven gesture generation using transformer-based generative models represents a rapidly advancing area within virtual human creation. However, existing models face significant challenges due to their quadratic time and space complexities, limiting scalability and efficiency. To address these limitations, we introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamb&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.16729v1-abstract-full').style.display = 'inline'; document.getElementById('2411.16729v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.16729v1-abstract-full" style="display: none;">
        Speech-driven gesture generation using transformer-based generative models represents a rapidly advancing area within virtual human creation. However, existing models face significant challenges due to their quadratic time and space complexities, limiting scalability and efficiency. To address these limitations, we introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamba-2 architecture. DiM-Gestor features a dual-component framework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping module, both built on the Mamba-2. The fuzzy feature extractor, integrated with a Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit, continuous speech features. These features are synthesized into a unified latent representation and then processed by the speech-to-gesture mapping module. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced Mamba-2 mechanism to uniformly apply transformations across all sequence tokens. This enables precise modeling of the nuanced interplay between speech features and gesture dynamics. We utilize a diffusion model to train and infer diverse gesture outputs. Extensive subjective and objective evaluations conducted on the newly released Chinese Co-Speech Gestures dataset corroborate the efficacy of our proposed model. Compared with Transformer-based architecture, the assessments reveal that our approach delivers competitive results and significantly reduces memory usage, approximately 2.4 times, and enhances inference speeds by 2 to 4 times. Additionally, we released the CCG dataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six styles across five scenarios) of 3D full-body skeleton gesture motion performed by professional Chinese TV broadcasters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.16729v1-abstract-full').style.display = 'none'; document.getElementById('2411.16729v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.15976">arXiv:2411.15976</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.15976">pdf</a>, <a href="https://arxiv.org/format/2411.15976">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DRIVE: Dual-Robustness via Information Variability and Entropic Consistency in Source-Free Unsupervised Domain Adaptation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+R">Ruiqiang Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lai%2C+S">Songning Lai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yijun Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiemin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yue%2C+Y">Yutao Yue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+L">Lei Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.15976v1-abstract-short" style="display: inline;">
        Adapting machine learning models to new domains without labeled data, especially when source data is inaccessible, is a critical challenge in applications like medical imaging, autonomous driving, and remote sensing. This task, known as Source-Free Unsupervised Domain Adaptation (SFUDA), involves adapting a pre-trained model to a target domain using only unlabeled target data, which can lead to is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15976v1-abstract-full').style.display = 'inline'; document.getElementById('2411.15976v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.15976v1-abstract-full" style="display: none;">
        Adapting machine learning models to new domains without labeled data, especially when source data is inaccessible, is a critical challenge in applications like medical imaging, autonomous driving, and remote sensing. This task, known as Source-Free Unsupervised Domain Adaptation (SFUDA), involves adapting a pre-trained model to a target domain using only unlabeled target data, which can lead to issues such as overfitting, underfitting, and poor generalization due to domain discrepancies and noise. Existing SFUDA methods often rely on single-model architectures, struggling with uncertainty and variability in the target domain. To address these challenges, we propose DRIVE (Dual-Robustness through Information Variability and Entropy), a novel SFUDA framework leveraging a dual-model architecture. The two models, initialized with identical weights, work in parallel to capture diverse target domain characteristics. One model is exposed to perturbations via projection gradient descent (PGD) guided by mutual information, focusing on high-uncertainty regions. We also introduce an entropy-aware pseudo-labeling strategy that adjusts label weights based on prediction uncertainty, ensuring the model focuses on reliable data while avoiding noisy regions. The adaptation process has two stages: the first aligns the models on stable features using a mutual information consistency loss, and the second dynamically adjusts the perturbation level based on the loss from the first stage, encouraging the model to explore a broader range of the target domain while preserving existing performance. This enhances generalization capabilities and robustness against interference. Evaluations on standard SFUDA benchmarks show that DRIVE consistently outperforms previous methods, delivering improved adaptation accuracy and stability across complex target domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15976v1-abstract-full').style.display = 'none'; document.getElementById('2411.15976v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.15537">arXiv:2411.15537</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.15537">pdf</a>, <a href="https://arxiv.org/format/2411.15537">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MUNBa: Machine Unlearning via Nash Bargaining
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Harandi%2C+M">Mehrtash Harandi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.15537v1-abstract-short" style="display: inline;">
        Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts/data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts, impeding MU algorithms f&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15537v1-abstract-full').style.display = 'inline'; document.getElementById('2411.15537v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.15537v1-abstract-full" style="display: none;">
        Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts/data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts, impeding MU algorithms from reaching optimal solutions. To address the gradient conflict issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain. To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto front, effectively avoiding the gradient conflicts. Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective. We evaluate our algorithm&#39;s effectiveness on a diverse set of tasks across image classification and image generation. Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving superior performance on several benchmarks. For example, in the challenging scenario of sample-wise forgetting, our algorithm approaches the gold standard retrain baseline. Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15537v1-abstract-full').style.display = 'none'; document.getElementById('2411.15537v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.15513">arXiv:2411.15513</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.15513">pdf</a>, <a href="https://arxiv.org/format/2411.15513">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SPA: Efficient User-Preference Alignment against Uncertainty in Medical Image Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+J">Jiayuan Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Junde Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+C">Cheng Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kamnitsas%2C+K">Konstantinos Kamnitsas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Noble%2C+A">Alison Noble</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.15513v1-abstract-short" style="display: inline;">
        Medical image segmentation data inherently contain uncertainty, often stemming from both imperfect image quality and variability in labeling preferences on ambiguous pixels, which depend on annotators&#39; expertise and the clinical context of the annotations. For instance, a boundary pixel might be labeled as tumor in diagnosis to avoid under-assessment of severity, but as normal tissue in radiothera&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15513v1-abstract-full').style.display = 'inline'; document.getElementById('2411.15513v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.15513v1-abstract-full" style="display: none;">
        Medical image segmentation data inherently contain uncertainty, often stemming from both imperfect image quality and variability in labeling preferences on ambiguous pixels, which depend on annotators&#39; expertise and the clinical context of the annotations. For instance, a boundary pixel might be labeled as tumor in diagnosis to avoid under-assessment of severity, but as normal tissue in radiotherapy to prevent damage to sensitive structures. As segmentation preferences vary across downstream applications, it is often desirable for an image segmentation model to offer user-adaptable predictions rather than a fixed output. While prior uncertainty-aware and interactive methods offer adaptability, they are inefficient at test time: uncertainty-aware models require users to choose from numerous similar outputs, while interactive models demand significant user input through click or box prompts to refine segmentation. To address these challenges, we propose \textbf{SPA}, a segmentation framework that efficiently adapts to diverse test-time preferences with minimal human interaction. By presenting users a select few, distinct segmentation candidates that best capture uncertainties, it reduces clinician workload in reaching the preferred segmentation. To accommodate user preference, we introduce a probabilistic mechanism that leverages user feedback to adapt model&#39;s segmentation preference. The proposed framework is evaluated on a diverse range of medical image segmentation tasks: color fundus images, CT, and MRI. It demonstrates 1) a significant reduction in clinician time and effort compared with existing interactive segmentation approaches, 2) strong adaptability based on human feedback, and 3) state-of-the-art image segmentation performance across diverse modalities and semantic labels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15513v1-abstract-full').style.display = 'none'; document.getElementById('2411.15513v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.15435">arXiv:2411.15435</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.15435">pdf</a>, <a href="https://arxiv.org/format/2411.15435">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What Makes a Scene ? Scene Graph-based Evaluation and Feedback for Controllable Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zuyao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinlin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+Z">Zhen Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C+W">Chang Wen Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.15435v1-abstract-short" style="display: inline;">
        While text-to-image generation has been extensively studied, generating images from scene graphs remains relatively underexplored, primarily due to challenges in accurately modeling spatial relationships and object interactions. To fill this gap, we introduce Scene-Bench, a comprehensive benchmark designed to evaluate and enhance the factual consistency in generating natural scenes. Scene-Bench co&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15435v1-abstract-full').style.display = 'inline'; document.getElementById('2411.15435v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.15435v1-abstract-full" style="display: none;">
        While text-to-image generation has been extensively studied, generating images from scene graphs remains relatively underexplored, primarily due to challenges in accurately modeling spatial relationships and object interactions. To fill this gap, we introduce Scene-Bench, a comprehensive benchmark designed to evaluate and enhance the factual consistency in generating natural scenes. Scene-Bench comprises MegaSG, a large-scale dataset of one million images annotated with scene graphs, facilitating the training and fair comparison of models across diverse and complex scenes. Additionally, we propose SGScore, a novel evaluation metric that leverages chain-of-thought reasoning capabilities of multimodal large language models (LLMs) to assess both object presence and relationship accuracy, offering a more effective measure of factual consistency than traditional metrics like FID and CLIPScore. Building upon this evaluation framework, we develop a scene graph feedback pipeline that iteratively refines generated images by identifying and correcting discrepancies between the scene graph and the image. Extensive experiments demonstrate that Scene-Bench provides a more comprehensive and effective evaluation framework compared to existing benchmarks, particularly for complex scene generation. Furthermore, our feedback strategy significantly enhances the factual consistency of image generation models, advancing the field of controllable image generation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15435v1-abstract-full').style.display = 'none'; document.getElementById('2411.15435v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.15426">arXiv:2411.15426</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.15426">pdf</a>, <a href="https://arxiv.org/format/2411.15426">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LDM-Morph: Latent diffusion model guided deformable image registration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gong%2C+K">Kuang Gong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.15426v1-abstract-short" style="display: inline;">
        Deformable image registration plays an essential role in various medical image tasks. Existing deep learning-based deformable registration frameworks primarily utilize convolutional neural networks (CNNs) or Transformers to learn features to predict the deformations. However, the lack of semantic information in the learned features limits the registration performance. Furthermore, the similarity m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15426v1-abstract-full').style.display = 'inline'; document.getElementById('2411.15426v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.15426v1-abstract-full" style="display: none;">
        Deformable image registration plays an essential role in various medical image tasks. Existing deep learning-based deformable registration frameworks primarily utilize convolutional neural networks (CNNs) or Transformers to learn features to predict the deformations. However, the lack of semantic information in the learned features limits the registration performance. Furthermore, the similarity metric of the loss function is often evaluated only in the pixel space, which ignores the matching of high-level anatomical features and can lead to deformation folding. To address these issues, in this work, we proposed LDM-Morph, an unsupervised deformable registration algorithm for medical image registration. LDM-Morph integrated features extracted from the latent diffusion model (LDM) to enrich the semantic information. Additionally, a latent and global feature-based cross-attention module (LGCA) was designed to enhance the interaction of semantic information from LDM and global information from multi-head self-attention operations. Finally, a hierarchical metric was proposed to evaluate the similarity of image pairs in both the original pixel space and latent-feature space, enhancing topology preservation while improving registration accuracy. Extensive experiments on four public 2D cardiac image datasets show that the proposed LDM-Morph framework outperformed existing state-of-the-art CNNs- and Transformers-based registration methods regarding accuracy and topology preservation with comparable computational efficiency. Our code is publicly available at https://github.com/wujiong-hub/LDM-Morph.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15426v1-abstract-full').style.display = 'none'; document.getElementById('2411.15426v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.15215">arXiv:2411.15215</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.15215">pdf</a>, <a href="https://arxiv.org/format/2411.15215">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        S$^2$ALM: Sequence-Structure Pre-trained Large Language Model for Comprehensive Antibody Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yin%2C+M">Mingze Yin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+H">Hanjing Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jialu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yiheng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhan%2C+Y">Yuxuan Zhan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kong%2C+Z">Zitai Kong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hongxia Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsieh%2C+C">Chang-Yu Hsieh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jintai Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hou%2C+T">Tingjun Hou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.15215v1-abstract-short" style="display: inline;">
        Antibodies safeguard our health through their precise and potent binding to specific antigens, demonstrating promising therapeutic efficacy in the treatment of numerous diseases, including COVID-19. Recent advancements in biomedical language models have shown the great potential to interpret complex biological structures and functions. However, existing antibody specific models have a notable limi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15215v1-abstract-full').style.display = 'inline'; document.getElementById('2411.15215v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.15215v1-abstract-full" style="display: none;">
        Antibodies safeguard our health through their precise and potent binding to specific antigens, demonstrating promising therapeutic efficacy in the treatment of numerous diseases, including COVID-19. Recent advancements in biomedical language models have shown the great potential to interpret complex biological structures and functions. However, existing antibody specific models have a notable limitation that they lack explicit consideration for antibody structural information, despite the fact that both 1D sequence and 3D structure carry unique and complementary insights into antibody behavior and functionality. This paper proposes Sequence-Structure multi-level pre-trained Antibody Language Model (S$^2$ALM), combining holistic sequential and structural information in one unified, generic antibody foundation model. We construct a hierarchical pre-training paradigm incorporated with two customized multi-level training objectives to facilitate the modeling of comprehensive antibody representations. S$^2$ALM&#39;s representation space uncovers inherent functional binding mechanisms, biological evolution properties and structural interaction patterns. Pre-trained over 75 million sequences and 11.7 million structures, S$^2$ALM can be adopted for diverse downstream tasks: accurately predicting antigen-antibody binding affinities, precisely distinguishing B cell maturation stages, identifying antibody crucial binding positions, and specifically designing novel coronavirus-binding antibodies. Remarkably, S$^2$ALM outperforms well-established and renowned baselines and sets new state-of-the-art performance across extensive antibody specific understanding and generation tasks. S$^2$ALM&#39;s ability to model comprehensive and generalized representations further positions its potential to advance real-world therapeutic antibody development, potentially addressing unmet academic, industrial, and clinical needs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.15215v1-abstract-full').style.display = 'none'; document.getElementById('2411.15215v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.14521">arXiv:2411.14521</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.14521">pdf</a>, <a href="https://arxiv.org/format/2411.14521">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MyTimeMachine: Personalized Facial Age Transformation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Qi%2C+L">Luchao Qi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiaye Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gong%2C+B">Bang Gong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+A+N">Annie N. Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jacobs%2C+D+W">David W. Jacobs</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sengupta%2C+R">Roni Sengupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.14521v1-abstract-short" style="display: inline;">
        Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person&#39;s appearance at the target age and thus need per&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.14521v1-abstract-full').style.display = 'inline'; document.getElementById('2411.14521v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.14521v1-abstract-full" style="display: none;">
        Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person&#39;s appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20$\sim$40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.14521v1-abstract-full').style.display = 'none'; document.getElementById('2411.14521v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://mytimemachine.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.13918">arXiv:2411.13918</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.13918">pdf</a>, <a href="https://arxiv.org/format/2411.13918">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantization without Tears
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+M">Minghao Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+H">Hao Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jie Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Junjie Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+K">Ke Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianxin Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.13918v2-abstract-short" style="display: inline;">
        Deep neural networks, while achieving remarkable success across diverse tasks, demand significant resources, including computation, GPU memory, bandwidth, storage, and energy. Network quantization, as a standard compression and acceleration technique, reduces storage costs and enables potential inference acceleration by discretizing network weights and activations into a finite set of integer valu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.13918v2-abstract-full').style.display = 'inline'; document.getElementById('2411.13918v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.13918v2-abstract-full" style="display: none;">
        Deep neural networks, while achieving remarkable success across diverse tasks, demand significant resources, including computation, GPU memory, bandwidth, storage, and energy. Network quantization, as a standard compression and acceleration technique, reduces storage costs and enables potential inference acceleration by discretizing network weights and activations into a finite set of integer values. However, current quantization methods are often complex and sensitive, requiring extensive task-specific hyperparameters, where even a single misconfiguration can impair model performance, limiting generality across different models and tasks. In this paper, we propose Quantization without Tears (QwT), a method that simultaneously achieves quantization speed, accuracy, simplicity, and generality. The key insight of QwT is to incorporate a lightweight additional structure into the quantized network to mitigate information loss during quantization. This structure consists solely of a small set of linear layers, keeping the method simple and efficient. More importantly, it provides a closed-form solution, allowing us to improve accuracy effortlessly under 2 minutes. Extensive experiments across various vision, language, and multimodal tasks demonstrate that QwT is both highly effective and versatile. In fact, our approach offers a robust solution for network quantization that combines simplicity, accuracy, and adaptability, which provides new insights for the design of novel quantization paradigms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.13918v2-abstract-full').style.display = 'none'; document.getElementById('2411.13918v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.12783">arXiv:2411.12783</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.12783">pdf</a>, <a href="https://arxiv.org/format/2411.12783">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Y">Yiming Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+X">Xun Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Ying Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+C">Chenyi Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+M">Miao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Ji Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.12783v1-abstract-short" style="display: inline;">
        The analysis of 3D medical images is crucial for modern healthcare, yet traditional task-specific models are becoming increasingly inadequate due to limited generalizability across diverse clinical scenarios. Multimodal large language models (MLLMs) offer a promising solution to these challenges. However, existing MLLMs have limitations in fully leveraging the rich, hierarchical information embedd&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12783v1-abstract-full').style.display = 'inline'; document.getElementById('2411.12783v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.12783v1-abstract-full" style="display: none;">
        The analysis of 3D medical images is crucial for modern healthcare, yet traditional task-specific models are becoming increasingly inadequate due to limited generalizability across diverse clinical scenarios. Multimodal large language models (MLLMs) offer a promising solution to these challenges. However, existing MLLMs have limitations in fully leveraging the rich, hierarchical information embedded in 3D medical images. Inspired by clinical practice, where radiologists focus on both 3D spatial structure and 2D planar content, we propose Med-2E3, a novel MLLM for 3D medical image analysis that integrates 3D and 2D encoders. To aggregate 2D features more effectively, we design a Text-Guided Inter-Slice (TG-IS) scoring module, which scores the attention of each 2D slice based on slice contents and task instructions. To the best of our knowledge, Med-2E3 is the first MLLM to integrate both 3D and 2D features for 3D medical image analysis. Experiments on a large-scale, open-source 3D medical multimodal benchmark demonstrate that Med-2E3 exhibits task-specific attention distribution and significantly outperforms current state-of-the-art models, with a 14% improvement in report generation and a 5% gain in medical visual question answering (VQA), highlighting the model&#39;s potential in addressing complex multimodal clinical tasks. The code will be released upon acceptance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12783v1-abstract-full').style.display = 'none'; document.getElementById('2411.12783v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.12629">arXiv:2411.12629</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.12629">pdf</a>, <a href="https://arxiv.org/format/2411.12629">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Astrophysics of Galaxies">astro-ph.GA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cosmology and Nongalactic Astrophysics">astro-ph.CO</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Garuda%2C+N">Nikhil Garuda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J+F">John F. Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nelson%2C+D">Dylan Nelson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pillepich%2C+A">Annalisa Pillepich</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.12629v1-abstract-short" style="display: inline;">
        Galaxies grow and evolve in dark matter halos. Because dark matter is not visible, galaxies&#39; halo masses ($\rm{M}_{\rm{halo}}$) must be inferred indirectly. We present a graph neural network (GNN) model for predicting $\rm{M}_{\rm{halo}}$ from stellar mass ($\rm{M}_{*}$) in simulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike traditional machine learning models like&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12629v1-abstract-full').style.display = 'inline'; document.getElementById('2411.12629v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.12629v1-abstract-full" style="display: none;">
        Galaxies grow and evolve in dark matter halos. Because dark matter is not visible, galaxies&#39; halo masses ($\rm{M}_{\rm{halo}}$) must be inferred indirectly. We present a graph neural network (GNN) model for predicting $\rm{M}_{\rm{halo}}$ from stellar mass ($\rm{M}_{*}$) in simulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike traditional machine learning models like random forests, our GNN captures the information-rich substructure of galaxy clusters by using spatial and kinematic relationships between galaxy neighbour. A GNN model trained on the TNG-Cluster dataset and independently tested on the TNG300 simulation achieves superior predictive performance compared to other baseline models we tested. Future work will extend this approach to different simulations and real observational datasets to further validate the GNN model&#39;s ability to generalise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12629v1-abstract-full').style.display = 'none'; document.getElementById('2411.12629v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 4 figures, accepted at the NeurIPS ML4PS 2024 workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.12588">arXiv:2411.12588</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.12588">pdf</a>, <a href="https://arxiv.org/format/2411.12588">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning To Sample the Meta-Paths for Social Event Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+C">Congbo Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiu%2C+Z">Zitai Qiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+S">Shan Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jia Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jian Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nakov%2C+P">Preslav Nakov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+Q+Z">Quan Z. Sheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.12588v1-abstract-short" style="display: inline;">
        Social media data is inherently rich, as it includes not only text content, but also users, geolocation, entities, temporal information, and their relationships. This data richness can be effectively modeled using heterogeneous information networks (HINs) as it can handle multiple types of nodes and relationships, allowing for a comprehensive representation of complex interactions within social da&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12588v1-abstract-full').style.display = 'inline'; document.getElementById('2411.12588v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.12588v1-abstract-full" style="display: none;">
        Social media data is inherently rich, as it includes not only text content, but also users, geolocation, entities, temporal information, and their relationships. This data richness can be effectively modeled using heterogeneous information networks (HINs) as it can handle multiple types of nodes and relationships, allowing for a comprehensive representation of complex interactions within social data. Meta-path-based methods use the sequences of relationships between different types of nodes in an HIN to capture the diverse and rich relationships within the social networks. However, the performance of social event detection methods is highly sensitive to the selection of meta-paths and existing meta-path based detectors either rely on human efforts or struggle to determining the effective meta-path set for model training and evaluation. In order to automatically discover the most important meta-paths, we propose a simple, yet effective, end-to-end Learning To Sample (LTS) framework for meta-path searching. Specifically, we build graphs that contain not only user profiles, textual content, and details about entities, but also the intricate relationships among them. The prioritized meta-paths, based on their importance, are sampled from the maintained distribution and their features are constructed before feeding into the social event detector. After picking up the top-ranked meta-paths, we streamline the exponential increment of meta-path combinations into a finite set of highly influential ones. The chosen meta-paths, along with their respective weights, are then used to train our social event detection model. As an alternative to social event detector training, we further propose an extra non-parametric evaluation process in order to determine the importance of each meta-path, which can further guide the paths sampling during model training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12588v1-abstract-full').style.display = 'none'; document.getElementById('2411.12588v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.12306">arXiv:2411.12306</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.12306">pdf</a>, <a href="https://arxiv.org/format/2411.12306">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diffusion Product Quantization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jie Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Hanxiao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianxin Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.12306v1-abstract-short" style="display: inline;">
        In this work, we explore the quantization of diffusion models in extreme compression regimes to reduce model size while maintaining performance. We begin by investigating classical vector quantization but find that diffusion models are particularly susceptible to quantization error, with the codebook size limiting generation quality. To address this, we introduce product quantization, which offers&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12306v1-abstract-full').style.display = 'inline'; document.getElementById('2411.12306v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.12306v1-abstract-full" style="display: none;">
        In this work, we explore the quantization of diffusion models in extreme compression regimes to reduce model size while maintaining performance. We begin by investigating classical vector quantization but find that diffusion models are particularly susceptible to quantization error, with the codebook size limiting generation quality. To address this, we introduce product quantization, which offers improved reconstruction precision and larger capacity -- crucial for preserving the generative capabilities of diffusion models. Furthermore, we propose a method to compress the codebook by evaluating the importance of each vector and removing redundancy, ensuring the model size remaining within the desired range. We also introduce an end-to-end calibration approach that adjusts assignments during the forward pass and optimizes the codebook using the DDPM loss. By compressing the model to as low as 1 bit (resulting in over 24 times reduction in model size), we achieve a balance between compression and quality. We apply our compression method to the DiT model on ImageNet and consistently outperform other quantization approaches, demonstrating competitive generative performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12306v1-abstract-full').style.display = 'none'; document.getElementById('2411.12306v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.12248">arXiv:2411.12248</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.12248">pdf</a>, <a href="https://arxiv.org/format/2411.12248">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Neuro-3D: Towards 3D Visual Decoding from EEG Signals
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+Z">Zhanqiang Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiamin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yonghao Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bu%2C+J">Jiahui Bu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mai%2C+W">Weijian Mai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Q">Qihao Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+W">Wanli Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+C">Chunfeng Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.12248v2-abstract-short" style="display: inline;">
        Human&#39;s perception of the visual world is shaped by the stereo processing of 3D information. Understanding how the brain perceives and processes 3D visual stimuli in the real world has been a longstanding endeavor in neuroscience. Towards this goal, we introduce a new neuroscience task: decoding 3D visual perception from EEG signals, a neuroimaging technique that enables real-time monitoring of ne&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12248v2-abstract-full').style.display = 'inline'; document.getElementById('2411.12248v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.12248v2-abstract-full" style="display: none;">
        Human&#39;s perception of the visual world is shaped by the stereo processing of 3D information. Understanding how the brain perceives and processes 3D visual stimuli in the real world has been a longstanding endeavor in neuroscience. Towards this goal, we introduce a new neuroscience task: decoding 3D visual perception from EEG signals, a neuroimaging technique that enables real-time monitoring of neural dynamics enriched with complex visual cues. To provide the essential benchmark, we first present EEG-3D, a pioneering dataset featuring multimodal analysis data and extensive EEG recordings from 12 subjects viewing 72 categories of 3D objects rendered in both videos and images. Furthermore, we propose Neuro-3D, a 3D visual decoding framework based on EEG signals. This framework adaptively integrates EEG features derived from static and dynamic stimuli to learn complementary and robust neural representations, which are subsequently utilized to recover both the shape and color of 3D objects through the proposed diffusion-based colored point cloud decoder. To the best of our knowledge, we are the first to explore EEG-based 3D visual decoding. Experiments indicate that Neuro-3D not only reconstructs colored 3D objects with high fidelity, but also learns effective neural representations that enable insightful brain region analysis. The dataset and associated code will be made publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12248v2-abstract-full').style.display = 'none'; document.getElementById('2411.12248v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.12126">arXiv:2411.12126</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.12126">pdf</a>, <a href="https://arxiv.org/format/2411.12126">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MMBind: Unleashing the Potential of Distributed and Heterogeneous Data for Multimodal Learning in IoT
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+X">Xiaomin Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jason Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kimura%2C+T">Tomoyoshi Kimura</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yihan Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Verma%2C+G">Gunjan Verma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abdelzaher%2C+T">Tarek Abdelzaher</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Srivastava%2C+M">Mani Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.12126v1-abstract-short" style="display: inline;">
        Multimodal sensing systems are increasingly prevalent in various real-world applications. Most existing multimodal learning approaches heavily rely on training with a large amount of complete multimodal data. However, such a setting is impractical in real-world IoT sensing applications where data is typically collected by distributed nodes with heterogeneous data modalities, and is also rarely lab&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12126v1-abstract-full').style.display = 'inline'; document.getElementById('2411.12126v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.12126v1-abstract-full" style="display: none;">
        Multimodal sensing systems are increasingly prevalent in various real-world applications. Most existing multimodal learning approaches heavily rely on training with a large amount of complete multimodal data. However, such a setting is impractical in real-world IoT sensing applications where data is typically collected by distributed nodes with heterogeneous data modalities, and is also rarely labeled. In this paper, we propose MMBind, a new framework for multimodal learning on distributed and heterogeneous IoT data. The key idea of MMBind is to construct a pseudo-paired multimodal dataset for model training by binding data from disparate sources and incomplete modalities through a sufficiently descriptive shared modality. We demonstrate that data of different modalities observing similar events, even captured at different times and locations, can be effectively used for multimodal training. Moreover, we propose an adaptive multimodal learning architecture capable of training models with heterogeneous modality combinations, coupled with a weighted contrastive learning approach to handle domain shifts among disparate data. Evaluations on ten real-world multimodal datasets highlight that MMBind outperforms state-of-the-art baselines under varying data incompleteness and domain shift, and holds promise for advancing multimodal foundation model training in IoT applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12126v1-abstract-full').style.display = 'none'; document.getElementById('2411.12126v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.11409">arXiv:2411.11409</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.11409">pdf</a>, <a href="https://arxiv.org/format/2411.11409">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yunong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eyzaguirre%2C+C">Cristobal Eyzaguirre</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+M">Manling Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khanna%2C+S">Shubh Khanna</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niebles%2C+J+C">Juan Carlos Niebles</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ravi%2C+V">Vineeth Ravi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mishra%2C+S">Saumitra Mishra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+W">Weiyu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.11409v1-abstract-short" style="display: inline;">
        Shape assembly is a ubiquitous task in daily life, integral for constructing complex 3D structures like IKEA furniture. While significant progress has been made in developing autonomous agents for shape assembly, existing datasets have not yet tackled the 4D grounding of assembly instructions in videos, essential for a holistic understanding of assembly in 3D space over time. We introduce IKEA Vid&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11409v1-abstract-full').style.display = 'inline'; document.getElementById('2411.11409v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.11409v1-abstract-full" style="display: none;">
        Shape assembly is a ubiquitous task in daily life, integral for constructing complex 3D structures like IKEA furniture. While significant progress has been made in developing autonomous agents for shape assembly, existing datasets have not yet tackled the 4D grounding of assembly instructions in videos, essential for a holistic understanding of assembly in 3D space over time. We introduce IKEA Video Manuals, a dataset that features 3D models of furniture parts, instructional manuals, assembly videos from the Internet, and most importantly, annotations of dense spatio-temporal alignments between these data modalities. To demonstrate the utility of IKEA Video Manuals, we present five applications essential for shape assembly: assembly plan generation, part-conditioned segmentation, part-conditioned pose estimation, video object segmentation, and furniture assembly based on instructional video manuals. For each application, we provide evaluation metrics and baseline methods. Through experiments on our annotated data, we highlight many challenges in grounding assembly instructions in videos to improve shape assembly, including handling occlusions, varying viewpoints, and extended assembly sequences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11409v1-abstract-full').style.display = 'none'; document.getElementById('2411.11409v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2024 Datasets and Benchmarks Track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.11361">arXiv:2411.11361</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.11361">pdf</a>, <a href="https://arxiv.org/format/2411.11361">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalable Autoregressive Monocular Depth Estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jinhong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jian Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+D">Dongqi Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Weiqiang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wentong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Danny Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jintai Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.11361v2-abstract-short" style="display: inline;">
        This paper shows that the autoregressive model is an effective and scalable monocular depth estimator. Our idea is simple: We tackle the monocular depth estimation (MDE) task with an autoregressive prediction paradigm, based on two core designs. First, our depth autoregressive model (DAR) treats the depth map of different resolutions as a set of tokens, and conducts the low-to-high resolution auto&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11361v2-abstract-full').style.display = 'inline'; document.getElementById('2411.11361v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.11361v2-abstract-full" style="display: none;">
        This paper shows that the autoregressive model is an effective and scalable monocular depth estimator. Our idea is simple: We tackle the monocular depth estimation (MDE) task with an autoregressive prediction paradigm, based on two core designs. First, our depth autoregressive model (DAR) treats the depth map of different resolutions as a set of tokens, and conducts the low-to-high resolution autoregressive objective with a patch-wise casual mask. Second, our DAR recursively discretizes the entire depth range into more compact intervals, and attains the coarse-to-fine granularity autoregressive objective in an ordinal-regression manner. By coupling these two autoregressive objectives, our DAR establishes new state-of-the-art (SOTA) on KITTI and NYU Depth v2 by clear margins. Further, our scalable approach allows us to scale the model up to 2.0B and achieve the best RMSE of 1.799 on the KITTI dataset (5% improvement) compared to 1.896 by the current SOTA (Depth Anything). DAR further showcases zero-shot generalization ability on unseen datasets. These results suggest that DAR yields superior performance with an autoregressive prediction paradigm, providing a promising approach to equip modern autoregressive large models (e.g., GPT-4o) with depth estimation capabilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11361v2-abstract-full').style.display = 'none'; document.getElementById('2411.11361v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.11192">arXiv:2411.11192</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.11192">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robot Metabolism: Towards machines that can grow by consuming other machines
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wyder%2C+P+M">Philippe Martin Wyder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bakhda%2C+R">Riyaan Bakhda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+M">Meiqi Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Booth%2C+Q+A">Quinn A. Booth</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Modi%2C+M+E">Matthew E. Modi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+A">Andrew Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+S">Simon Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiahao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Patel%2C+P">Priya Patel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kasumi%2C+R+T">Robert T. Kasumi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yi%2C+D">David Yi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garg%2C+N+N">Nihar Niraj Garg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jhunjhunwala%2C+P">Pranav Jhunjhunwala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bhutoria%2C+S">Siddharth Bhutoria</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tong%2C+E+H">Evan H. Tong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yuhang Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goldfeder%2C+J">Judah Goldfeder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mustel%2C+O">Omer Mustel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+D">Donghan Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lipson%2C+H">Hod Lipson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.11192v1-abstract-short" style="display: inline;">
        Biological lifeforms can heal, grow, adapt, and reproduce -- abilities essential for sustained survival and development. In contrast, robots today are primarily monolithic machines with limited ability to self-repair, physically develop, or incorporate material from their environments. A key challenge to such physical adaptation has been that while robot minds are rapidly evolving new behaviors th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11192v1-abstract-full').style.display = 'inline'; document.getElementById('2411.11192v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.11192v1-abstract-full" style="display: none;">
        Biological lifeforms can heal, grow, adapt, and reproduce -- abilities essential for sustained survival and development. In contrast, robots today are primarily monolithic machines with limited ability to self-repair, physically develop, or incorporate material from their environments. A key challenge to such physical adaptation has been that while robot minds are rapidly evolving new behaviors through AI, their bodies remain closed systems, unable to systematically integrate new material to grow or heal. We argue that open-ended physical adaptation is only possible when robots are designed using only a small repertoire of simple modules. This allows machines to mechanically adapt by consuming parts from other machines or their surroundings and shedding broken components. We demonstrate this principle using a truss modular robot platform composed of one-dimensional actuated bars. We show how robots in this space can grow bigger, faster, and more capable by consuming materials from their environment and from other robots. We suggest that machine metabolic processes akin to the one demonstrated here will be an essential part of any sustained future robot ecology.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11192v1-abstract-full').style.display = 'none'; document.getElementById('2411.11192v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Manuscript combined with Supplementary Materials File for arXiv submission. Submitting to Journal and will update external DOI once available</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          70-01; 68-02
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.6; H.4; H.m; I.m; B.m
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.11148">arXiv:2411.11148</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.11148">pdf</a>, <a href="https://arxiv.org/format/2411.11148">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TabDeco: A Comprehensive Contrastive Framework for Decoupled Representations in Tabular Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+S">Suiyao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yunxiao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+C">Cheng Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+T">Tianpei Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cociorva%2C+D">Daniel Cociorva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sharps%2C+M">Michael Sharps</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Levasseur%2C+C">Cecile Levasseur</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brunzell%2C+H">Hakan Brunzell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.11148v1-abstract-short" style="display: inline;">
        Representation learning is a fundamental aspect of modern artificial intelligence, driving substantial improvements across diverse applications. While selfsupervised contrastive learning has led to significant advancements in fields like computer vision and natural language processing, its adaptation to tabular data presents unique challenges. Traditional approaches often prioritize optimizing mod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11148v1-abstract-full').style.display = 'inline'; document.getElementById('2411.11148v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.11148v1-abstract-full" style="display: none;">
        Representation learning is a fundamental aspect of modern artificial intelligence, driving substantial improvements across diverse applications. While selfsupervised contrastive learning has led to significant advancements in fields like computer vision and natural language processing, its adaptation to tabular data presents unique challenges. Traditional approaches often prioritize optimizing model architecture and loss functions but may overlook the crucial task of constructing meaningful positive and negative sample pairs from various perspectives like feature interactions, instance-level patterns and batch-specific contexts. To address these challenges, we introduce TabDeco, a novel method that leverages attention-based encoding strategies across both rows and columns and employs contrastive learning framework to effectively disentangle feature representations at multiple levels, including features, instances and data batches. With the innovative feature decoupling hierarchies, TabDeco consistently surpasses existing deep learning methods and leading gradient boosting algorithms, including XG-Boost, CatBoost, and LightGBM, across various benchmark tasks, underscoring its effectiveness in advancing tabular data representation learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11148v1-abstract-full').style.display = 'none'; document.getElementById('2411.11148v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.10741">arXiv:2411.10741</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.10741">pdf</a>, <a href="https://arxiv.org/format/2411.10741">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chou%2C+Y">Yuhong Chou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+M">Man Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+K">Kexin Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+Y">Yuqi Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+R">Ruijie Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Y">Yiran Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+Y">Yu Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jibin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+B">Bo Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+G">Guoqi Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.10741v1-abstract-short" style="display: inline;">
        Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax atten&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10741v1-abstract-full').style.display = 'inline'; document.getElementById('2411.10741v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.10741v1-abstract-full" style="display: none;">
        Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax attention from a theoretical perspective. We start by unifying existing linear complexity models as the linear attention form and then identify three conditions for the optimal linear attention design: 1) Dynamic memory ability; 2) Static approximation ability; 3) Least parameter approximation. We find that none of the current linear models meet all three conditions, resulting in suboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions. Our experiments on Multi-Query Associative Recall (MQAR) task, language modeling, image classification, and Long-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than the existing linear models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10741v1-abstract-full').style.display = 'none'; document.getElementById('2411.10741v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.10340">arXiv:2411.10340</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.10340">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Domain Adaptation-based Edge Computing for Cross-Conditions Fault Diagnosis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yanzhi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jinhong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Z">Ziyang Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Q">Qi Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.10340v1-abstract-short" style="display: inline;">
        Fault diagnosis technology supports the healthy operation of mechanical equipment. However, the variations conditions during the operation of mechanical equipment lead to significant disparities in data distribution, posing challenges to fault diagnosis. Furthermore, when deploying applications, traditional methods often encounter issues such as latency and data security. Therefore, conducting fau&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10340v1-abstract-full').style.display = 'inline'; document.getElementById('2411.10340v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.10340v1-abstract-full" style="display: none;">
        Fault diagnosis technology supports the healthy operation of mechanical equipment. However, the variations conditions during the operation of mechanical equipment lead to significant disparities in data distribution, posing challenges to fault diagnosis. Furthermore, when deploying applications, traditional methods often encounter issues such as latency and data security. Therefore, conducting fault diagnosis and deploying application methods under cross-operating conditions holds significant value. This paper proposes a domain adaptation-based lightweight fault diagnosis framework for edge computing scenarios. Incorporating the local maximum mean discrepancy into knowledge transfer aligns the feature distributions of different domains in a high-dimensional feature space, to discover a common feature space across domains. The acquired fault diagnosis expertise from the cloud-model is transferred to the lightweight edge-model using adaptation knowledge transfer methods. While ensuring real-time diagnostic capabilities, accurate fault diagnosis is achieved across working conditions. We conducted validation experiments on the NVIDIA Jetson Xavier NX kit. In terms of diagnostic performance, the proposed method significantly improved diagnostic accuracy, with average increases of 34.44% and 17.33% compared to the comparison method, respectively. Regarding lightweight effectiveness, proposed method achieved an average inference speed increase of 80.47%. Additionally, compared to the cloud-model, the parameter count of the edge-model decreased by 96.37%, while the Flops decreased by 83.08%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10340v1-abstract-full').style.display = 'none'; document.getElementById('2411.10340v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">28 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.10191">arXiv:2411.10191</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.10191">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Atmospheric and Oceanic Physics">physics.ao-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FengWu-W2S: A deep learning model for seamless weather-to-subseasonal forecast of global atmosphere
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ling%2C+F">Fenghua Ling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+K">Kang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiye Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+T">Tao Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+J">Jing-Jia Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+W">Wanli Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bai%2C+L">Lei Bai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.10191v2-abstract-short" style="display: inline;">
        Seamless forecasting that produces warning information at continuum timescales based on only one system is a long-standing pursuit for weather-climate service. While the rapid advancement of deep learning has induced revolutionary changes in classical forecasting field, current efforts are still focused on building separate AI models for weather and climate forecasts. To explore the seamless forec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10191v2-abstract-full').style.display = 'inline'; document.getElementById('2411.10191v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.10191v2-abstract-full" style="display: none;">
        Seamless forecasting that produces warning information at continuum timescales based on only one system is a long-standing pursuit for weather-climate service. While the rapid advancement of deep learning has induced revolutionary changes in classical forecasting field, current efforts are still focused on building separate AI models for weather and climate forecasts. To explore the seamless forecasting ability based on one AI model, we propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the FengWu global weather forecast model and incorporates an ocean-atmosphere-land coupling structure along with a diverse perturbation strategy. FengWu-W2S can generate 6-hourly atmosphere forecasts extending up to 42 days through an autoregressive and seamless manner. Our hindcast results demonstrate that FengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead, enhancing predictive capabilities for global surface air temperature, precipitation, geopotential height and intraseasonal signals such as the Madden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover, our ablation experiments on forecast error growth from daily to seasonal timescales reveal potential pathways for developing AI-based integrated system for seamless weather-climate forecasting in the future.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10191v2-abstract-full').style.display = 'none'; document.getElementById('2411.10191v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages,8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.10169">arXiv:2411.10169</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.10169">pdf</a>, <a href="https://arxiv.org/format/2411.10169">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Definition and Detection of Centralization Defects in Smart Contracts
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Z">Zewei Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jiachi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Weizhe Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Z">Zibin Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.10169v1-abstract-short" style="display: inline;">
        In recent years, security incidents stemming from centralization defects in smart contracts have led to substantial financial losses. A centralization defect refers to any error, flaw, or fault in a smart contract&#39;s design or development stage that introduces a single point of failure. Such defects allow a specific account or user to disrupt the normal operations of smart contracts, potentially ca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10169v1-abstract-full').style.display = 'inline'; document.getElementById('2411.10169v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.10169v1-abstract-full" style="display: none;">
        In recent years, security incidents stemming from centralization defects in smart contracts have led to substantial financial losses. A centralization defect refers to any error, flaw, or fault in a smart contract&#39;s design or development stage that introduces a single point of failure. Such defects allow a specific account or user to disrupt the normal operations of smart contracts, potentially causing malfunctions or even complete project shutdowns. Despite the significance of this issue, most current smart contract analyses overlook centralization defects, focusing primarily on other types of defects. To address this gap, our paper introduces six types of centralization defects in smart contracts by manually analyzing 597 Stack Exchange posts and 117 audit reports. For each defect, we provide a detailed description and code examples to illustrate its characteristics and potential impacts. Additionally, we introduce a tool named CDRipper (Centralization Defects Ripper) designed to identify the defined centralization defects. Specifically, CDRipper constructs a permission dependency graph (PDG) and extracts the permission dependencies of functions from the source code of smart contracts. It then detects the sensitive operations in functions and identifies centralization defects based on predefined patterns. We conduct a large-scale experiment using CDRipper on 244,424 real-world smart contracts and evaluate the results based on a manually labeled dataset. Our findings reveal that 82,446 contracts contain at least one of the six centralization defects, with our tool achieving an overall precision of 93.7%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10169v1-abstract-full').style.display = 'none'; document.getElementById('2411.10169v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.09895">arXiv:2411.09895</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.09895">pdf</a>, <a href="https://arxiv.org/format/2411.09895">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploiting Cross-Layer Vulnerabilities: Off-Path Attacks on the TCP/IP Protocol Suite
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+X">Xuewei Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Qi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+K">Kun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+K">Ke Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianping Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.09895v2-abstract-short" style="display: inline;">
        After more than 40 years of development, the fundamental TCP/IP protocol suite, serving as the backbone of the Internet, is widely recognized for having achieved an elevated level of robustness and security. Distinctively, we take a new perspective to investigate the security implications of cross-layer interactions within the TCP/IP protocol suite caused by ICMP error messages. Through a comprehe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09895v2-abstract-full').style.display = 'inline'; document.getElementById('2411.09895v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.09895v2-abstract-full" style="display: none;">
        After more than 40 years of development, the fundamental TCP/IP protocol suite, serving as the backbone of the Internet, is widely recognized for having achieved an elevated level of robustness and security. Distinctively, we take a new perspective to investigate the security implications of cross-layer interactions within the TCP/IP protocol suite caused by ICMP error messages. Through a comprehensive analysis of interactions among Wi-Fi, IP, ICMP, UDP, and TCP due to ICMP errors, we uncover several significant vulnerabilities, including information leakage, desynchronization, semantic gaps, and identity spoofing. These vulnerabilities can be exploited by off-path attackers to manipulate network traffic stealthily, affecting over 20% of popular websites and more than 89% of public Wi-Fi networks, thus posing risks to the Internet. By responsibly disclosing these vulnerabilities to affected vendors and proposing effective countermeasures, we enhance the robustness of the TCP/IP protocol suite, receiving acknowledgments from well-known organizations such as the Linux community, the OpenWrt community, the FreeBSD community, Wi-Fi Alliance, Qualcomm, HUAWEI, China Telecom, Alibaba, and H3C.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09895v2-abstract-full').style.display = 'none'; document.getElementById('2411.09895v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.09705">arXiv:2411.09705</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.09705">pdf</a>, <a href="https://arxiv.org/format/2411.09705">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Residual Multi-Task Learner for Applied Ranking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+C">Cong Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+K">Kun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiahua Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yizhou Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huzhang%2C+G">Guangda Huzhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ni%2C+Y">Yabo Ni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+A">Anxiang Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Z">Zhiming Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.09705v1-abstract-short" style="display: inline;">
        Modern e-commerce platforms rely heavily on modeling diverse user feedback to provide personalized services. Consequently, multi-task learning has become an integral part of their ranking systems. However, existing multi-task learning methods encounter two main challenges: some lack explicit modeling of task relationships, resulting in inferior performance, while others have limited applicability&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09705v1-abstract-full').style.display = 'inline'; document.getElementById('2411.09705v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.09705v1-abstract-full" style="display: none;">
        Modern e-commerce platforms rely heavily on modeling diverse user feedback to provide personalized services. Consequently, multi-task learning has become an integral part of their ranking systems. However, existing multi-task learning methods encounter two main challenges: some lack explicit modeling of task relationships, resulting in inferior performance, while others have limited applicability due to being computationally intensive, having scalability issues, or relying on strong assumptions. To address these limitations and better fit our real-world scenario, pre-rank in Shopee Search, we introduce in this paper ResFlow, a lightweight multi-task learning framework that enables efficient cross-task information sharing via residual connections between corresponding layers of task networks. Extensive experiments on datasets from various scenarios and modalities demonstrate its superior performance and adaptability over state-of-the-art methods. The online A/B tests in Shopee Search showcase its practical value in large-scale industrial applications, evidenced by a 1.29% increase in OPU (order-per-user) without additional system latency. ResFlow is now fully deployed in the pre-rank module of Shopee Search. To facilitate efficient online deployment, we propose a novel offline metric Weighted Recall@K, which aligns well with our online metric OPU, addressing the longstanding online-offline metric misalignment issue. Besides, we propose to fuse scores from the multiple tasks additively when ranking items, which outperforms traditional multiplicative fusion. The code is released at https://github.com/BrunoTruthAlliance/ResFlow
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09705v1-abstract-full').style.display = 'none'; document.getElementById('2411.09705v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.09489">arXiv:2411.09489</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.09489">pdf</a>, <a href="https://arxiv.org/ps/2411.09489">ps</a>, <a href="https://arxiv.org/format/2411.09489">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Positive Focusing is Directly Useful
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Accattoli%2C+B">Beniamino Accattoli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jui-Hsuan Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.09489v1-abstract-short" style="display: inline;">
        Recently, Miller and Wu introduced the positive $$-calculus, a call-by-value $$-calculus with sharing obtained by assigning proof terms to the positively polarized focused proofs for minimal intuitionistic logic. The positive $$-calculus stands out among $$-calculi with sharing for a compactness property related to the sharing of variables. We show that -- thanks to compactness -- the positive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09489v1-abstract-full').style.display = 'inline'; document.getElementById('2411.09489v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.09489v1-abstract-full" style="display: none;">
        Recently, Miller and Wu introduced the positive $$-calculus, a call-by-value $$-calculus with sharing obtained by assigning proof terms to the positively polarized focused proofs for minimal intuitionistic logic. The positive $$-calculus stands out among $$-calculi with sharing for a compactness property related to the sharing of variables. We show that -- thanks to compactness -- the positive calculus neatly captures the core of useful sharing, a technique for the study of reasonable time cost models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09489v1-abstract-full').style.display = 'none'; document.getElementById('2411.09489v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper for the proceedings of MFPS 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.09371">arXiv:2411.09371</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.09371">pdf</a>, <a href="https://arxiv.org/format/2411.09371">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DSCformer: A Dual-Branch Network Integrating Enhanced Dynamic Snake Convolution and SegFormer for Crack Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+K">Kaiwei Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+I">I-Ming Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jing Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.09371v1-abstract-short" style="display: inline;">
        In construction quality monitoring, accurately detecting and segmenting cracks in concrete structures is paramount for safety and maintenance. Current convolutional neural networks (CNNs) have demonstrated strong performance in crack segmentation tasks, yet they often struggle with complex backgrounds and fail to capture fine-grained tubular structures fully. In contrast, Transformers excel at cap&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09371v1-abstract-full').style.display = 'inline'; document.getElementById('2411.09371v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.09371v1-abstract-full" style="display: none;">
        In construction quality monitoring, accurately detecting and segmenting cracks in concrete structures is paramount for safety and maintenance. Current convolutional neural networks (CNNs) have demonstrated strong performance in crack segmentation tasks, yet they often struggle with complex backgrounds and fail to capture fine-grained tubular structures fully. In contrast, Transformers excel at capturing global context but lack precision in detailed feature extraction. We introduce DSCformer, a novel hybrid model that integrates an enhanced Dynamic Snake Convolution (DSConv) with a Transformer architecture for crack segmentation to address these challenges. Our key contributions include the enhanced DSConv through a pyramid kernel for adaptive offset computation and a simultaneous bi-directional learnable offset iteration, significantly improving the model&#39;s performance to capture intricate crack patterns. Additionally, we propose a Weighted Convolutional Attention Module (WCAM), which refines channel attention, allowing for more precise and adaptive feature attention. We evaluate DSCformer on the Crack3238 and FIND datasets, achieving IoUs of 59.22\% and 87.24\%, respectively. The experimental results suggest that our DSCformer outperforms state-of-the-art methods across different datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.09371v1-abstract-full').style.display = 'none'; document.getElementById('2411.09371v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.07076">arXiv:2411.07076</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.07076">pdf</a>, <a href="https://arxiv.org/format/2411.07076">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Y">Yichen He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yuan Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianchao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Hanchong Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yuchen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Le%2C+R">Ruicheng Le</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.07076v1-abstract-short" style="display: inline;">
        Existing large vision-language models (LVLMs) are largely limited to processing short, seconds-long videos and struggle with generating coherent descriptions for extended video spanning minutes or more. Long video description introduces new challenges, such as plot-level consistency across descriptions. To address these, we figure out audio-visual character identification, matching character names&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.07076v1-abstract-full').style.display = 'inline'; document.getElementById('2411.07076v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.07076v1-abstract-full" style="display: none;">
        Existing large vision-language models (LVLMs) are largely limited to processing short, seconds-long videos and struggle with generating coherent descriptions for extended video spanning minutes or more. Long video description introduces new challenges, such as plot-level consistency across descriptions. To address these, we figure out audio-visual character identification, matching character names to each dialogue, as a key factor. We propose StoryTeller, a system for generating dense descriptions of long videos, incorporating both low-level visual concepts and high-level plot information. StoryTeller uses a multimodal large language model that integrates visual, audio, and text modalities to perform audio-visual character identification on minute-long video clips. The results are then fed into a LVLM to enhance consistency of video description. We validate our approach on movie description tasks and introduce MovieStory101, a dataset with dense descriptions for three-minute movie clips. To evaluate long video descriptions, we create MovieQA, a large set of multiple-choice questions for the MovieStory101 test set. We assess descriptions by inputting them into GPT-4 to answer these questions, using accuracy as an automatic evaluation metric. Experiments show that StoryTeller outperforms all open and closed-source baselines on MovieQA, achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and demonstrating a +15.56% advantage in human side-by-side evaluations. Additionally, incorporating audio-visual character identification from StoryTeller improves the performance of all video description models, with Gemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%, respectively, in accuracy on MovieQA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.07076v1-abstract-full').style.display = 'none'; document.getElementById('2411.07076v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.07021">arXiv:2411.07021</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.07021">pdf</a>, <a href="https://arxiv.org/format/2411.07021">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Ziwei Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+L">Liang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Qian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianghua Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+G">Guangxu Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.07021v2-abstract-short" style="display: inline;">
        Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in informat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.07021v2-abstract-full').style.display = 'inline'; document.getElementById('2411.07021v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.07021v2-abstract-full" style="display: none;">
        Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs&#39; semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever.
  To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.07021v2-abstract-full').style.display = 'none'; document.getElementById('2411.07021v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Wu%2C+J&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>